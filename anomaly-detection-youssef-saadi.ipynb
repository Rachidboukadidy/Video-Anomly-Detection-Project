{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Clone the repository\n!git clone https://github.com/rishikksh20/ViViT-pytorch.git \n# Step 2: Change directory to the cloned repo\n%cd ViViT-pytorch\n\n# Step 4: Run scripts or modify files\n!python module.py vivit.py  # Example script to run\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## setup libraries & device","metadata":{}},{"cell_type":"code","source":"import os, sys\nimport natsort # For number sorting\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport natsort\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import einsum\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom module import Attention, PreNorm, FeedForward\nimport cv2\nfrom google.colab.patches import cv2_imshow # colabÏóêÏÑú cv2.imshow ÏÇ¨Ïö© Î∂àÍ∞Ä\nsys.path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\nprint(device)\nprint(torch.cuda.is_available())  # Should print True if GPU is enabled\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nprint(torch.cuda.device_count())  # Nombre de GPUs\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Pretraitement de dataloader pour surveillance camera","metadata":{}},{"cell_type":"code","source":"class DashcamDataset(Dataset):\n    def __init__(self, root_dir, sequence_length=4, transform=None, overlap=True):\n        self.root_dir = root_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n\n        # Parcourir chaque vid√©o s√©par√©ment\n        for video_folder in sorted(os.listdir(root_dir)):\n            video_path = os.path.join(root_dir, video_folder)\n            if os.path.isdir(video_path):\n                frames = sorted(\n                    [f for f in os.listdir(video_path) if f.startswith('frame_')],\n                    key=lambda x: int(x.split('_')[1].split('.')[0]))  # Trie par num√©ro (001 -> 1)\n                frames = [os.path.join(video_path, f) for f in frames]\n\n                # G√©n√©rer les s√©quences pour cette vid√©o\n                if overlap:\n                    for i in range(len(frames) - sequence_length):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n                else:\n                    for i in range(0, len(frames) - sequence_length, sequence_length + 1):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        for img_path in frame_paths:\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            if self.transform:\n                img = self.transform(img)\n            images.append(img)\n\n        input_frames = torch.stack(images[:self.sequence_length], dim=0)\n        target_frame = images[self.sequence_length]\n        target_name = os.path.basename(frame_paths[-1])\n        return input_frames, target_frame, target_name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# D√©finition des transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalisation [-1, 1]\n])\n\n# Cr√©ation du Dataset et du DataLoader avec la nouvelle classe am√©lior√©e\ntrain_dataset = DashcamDataset(\n    root_dir=\"/kaggle/input/surveillance-camera-fightnofight/Surveillance Camera Fight Dataset/Train\",\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=False,  # Garder False pour l'entra√Ænement pour maintenir l'ordre temporel\n    num_workers=4,\n    pin_memory=True,\n)\n\ntest_dataset = DashcamDataset(\n    root_dir=\"/kaggle/input/surveillance-camera-fightnofight/Surveillance Camera Fight Dataset/Test\",\n    sequence_length=4,\n    transform=transform,\n    overlap=True  # Param√®tre valide pour DashcamDataset\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,  # D√©sactiv√© pour maintenir l'ordre temporel\n    num_workers=4,\n    pin_memory=True,\n)\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef display_sequence_with_target(input_frames, target_frame, sequence_idx, target_name):\n    \"\"\"\n    Affiche les frames d'entr√©e et la frame cible avec des informations d√©taill√©es.\n    \"\"\"\n    # Convertir et pr√©parer les images\n    input_frames = input_frames.cpu().numpy().transpose(0, 2, 3, 1)\n    target_frame = target_frame.cpu().numpy().transpose(1, 2, 0)\n    input_frames = (input_frames + 1) / 2\n    target_frame = (target_frame + 1) / 2\n\n    # Extraire les num√©ros de frame\n    target_num = target_name.split('_')[1].split('.')[0]\n    input_nums = [str(int(target_num) - len(input_frames) + i) for i in range(len(input_frames))]\n    \n    # Cr√©er la figure\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(\n        f\"Sequence {sequence_idx + 1}\\n\"\n        f\"Input frames ({len(input_frames)}): {', '.join(input_nums)}\\n\"\n        f\"Target frame: {target_num}\",\n        fontsize=12, y=1.05\n    )\n    \n    # Afficher les frames d'entr√©e\n    for i in range(len(input_frames)):\n        plt.subplot(1, len(input_frames) + 1, i + 1)\n        plt.imshow(input_frames[i])\n        plt.title(f\"Frame {input_nums[i]}\")\n        plt.axis('off')\n    \n    # Afficher la cible\n    plt.subplot(1, len(input_frames) + 1, len(input_frames) + 1)\n    plt.imshow(target_frame)\n    plt.title(f\"Target: {target_num}\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Param√®tre configurable - CHANGEZ ICI LE NOMBRE DE BATCHS √Ä AFFICHER\nNUM_BATCHES_TO_DISPLAY = 3  # ‚Üê Modifiez ce nombre selon vos besoins\n\n# It√©ration sur le DataLoader\nfor batch_idx, (input_frames, target_frames, target_names) in enumerate(train_dataloader):\n    if batch_idx >= NUM_BATCHES_TO_DISPLAY:\n        break\n        \n    print(f\"\\n{'='*40}\")\n    print(f\"=== BATCH {batch_idx + 1}/{NUM_BATCHES_TO_DISPLAY} ===\")\n    print(f\"{'='*40}\")\n    print(f\"Nombre total de s√©quences dans ce batch: {input_frames.shape[0]}\")\n    \n    for seq_idx in range(input_frames.shape[0]):\n        current_input = input_frames[seq_idx]\n        current_target = target_frames[seq_idx]\n        current_target_name = target_names[seq_idx]\n        \n        # Affichage console\n        target_num = current_target_name.split('_')[1].split('.')[0].zfill(3)\n        input_nums = [str(int(target_num) - len(current_input) + i).zfill(3) for i in range(len(current_input))]\n        \n        print(f\"\\nS√©quence {seq_idx + 1}:\")\n        print(f\"‚Ä¢ Frames d'entr√©e ({len(input_nums)}): {', '.join(input_nums)}\")\n        print(f\"‚Ä¢ Frame cible: {target_num}\")\n        \n        # Affichage graphique\n        display_sequence_with_target(current_input, current_target, seq_idx, current_target_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pretraitement de Dataloader pour Avune","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass AvenueDataset(Dataset):\n    def __init__(self, root_dir, sequence_length=4, transform=None, overlap=True):\n        self.root_dir = root_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n        \n        # Parcours des dossiers de s√©quences (02, 03, etc.)\n        for seq_folder in sorted(os.listdir(root_dir)):\n            seq_path = os.path.join(root_dir, seq_folder)\n            if os.path.isdir(seq_path):\n                # Liste des images tri√©es par num√©ro\n                frames = sorted([f for f in os.listdir(seq_path) if f.endswith('.jpg')],\n                               key=lambda x: int(x.split('.')[0]))\n                frames = [os.path.join(seq_path, f) for f in frames]\n                \n                # Cr√©ation des s√©quences\n                if overlap:\n                    for i in range(len(frames) - sequence_length):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n                else:\n                    for i in range(0, len(frames) - sequence_length, sequence_length + 1):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        \n        for img_path in frame_paths:\n            img = cv2.imread(img_path)\n            if img is None:\n                raise ValueError(f\"Impossible de charger l'image: {img_path}\")\n                \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            \n            if self.transform:\n                img = self.transform(img)\n                \n            images.append(img)\n        \n        input_frames = torch.stack(images[:self.sequence_length], dim=0)\n        target_frame = images[self.sequence_length]\n        target_name = os.path.basename(frame_paths[-1])\n        \n        return input_frames, target_frame, target_name\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Chemins des donn√©es - A ADAPTER selon votre structure exacte\ntrain_root = \"/kaggle/input/avunue-rachid/avenue/training\"\ntest_root = \"/kaggle/input/avunue-rachid/avenue/testing\"\n\n# Datasets\ntrain_dataset = AvenueDataset(\n    root_dir=train_root,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\ntest_dataset = AvenueDataset(\n    root_dir=test_root,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\n# DataLoaders\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ndef display_sequence(input_frames, target_frame, sequence_idx, target_name):\n    \"\"\"\n    Affiche une s√©quence de frames et la frame cible\n    \"\"\"\n    # Conversion des tenseurs en numpy arrays\n    input_frames = input_frames.cpu().numpy()\n    target_frame = target_frame.cpu().numpy()\n    \n    # R√©organisation des dimensions et d√©normalisation\n    input_frames = np.transpose(input_frames, (0, 2, 3, 1))\n    target_frame = np.transpose(target_frame, (1, 2, 0))\n    \n    input_frames = (input_frames + 1) / 2\n    target_frame = (target_frame + 1) / 2\n    \n    # Num√©ros des frames\n    target_num = target_name.split('.')[0]\n    input_nums = [str(int(target_num) - len(input_frames) + i).zfill(4) \n                 for i in range(len(input_frames))]\n    \n    # Cr√©ation de la figure\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(f\"S√©quence {sequence_idx + 1} - Target: {target_num}\", fontsize=14)\n    \n    # Affichage des frames d'entr√©e\n    for i, frame in enumerate(input_frames):\n        plt.subplot(1, len(input_frames) + 1, i + 1)\n        plt.imshow(frame)\n        plt.title(f\"Frame {input_nums[i]}\")\n        plt.axis('off')\n    \n    # Affichage de la cible\n    plt.subplot(1, len(input_frames) + 1, len(input_frames) + 1)\n    plt.imshow(target_frame)\n    plt.title(f\"Target {target_num}\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualisation des premi√®res s√©quences\nNUM_BATCHES_TO_DISPLAY = 2\n\nfor batch_idx, (inputs, targets, names) in enumerate(train_dataloader):\n    if batch_idx >= NUM_BATCHES_TO_DISPLAY:\n        break\n    \n    print(f\"\\n=== Batch {batch_idx + 1} ===\")\n    print(f\"Input shape: {inputs.shape}\")\n    print(f\"Target shape: {targets.shape}\")\n    print(f\"Target names: {names}\")\n    \n    for seq_idx in range(inputs.shape[0]):\n        display_sequence(\n            inputs[seq_idx], \n            targets[seq_idx], \n            seq_idx, \n            names[seq_idx]\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Definition de model","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        self.norm = nn.LayerNorm(dim)\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return self.norm(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ViViT(nn.Module):\n    def __init__(self, image_size, patch_size, num_frames, dim=512, depth=4, heads=3, pool='cls', in_channels=512,\n                 dim_head=64, dropout=0., emb_dropout=0., scale_dim=4, depth_spatial=3, depth_temporal=1):\n        super().__init__()\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_frames = num_frames\n        self.in_channels = in_channels\n        self.dim = dim\n        self.depth_spatial = depth_spatial\n        self.depth_temporal = depth_temporal\n\n        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n        self.num_patches = (image_size // patch_size) ** 2\n        self.patch_dim = in_channels * patch_size ** 2  # chaque patch aplati\n\n        # üîÑ PATCH EMBEDDING\n        \n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(self.patch_dim, self.dim)  # (b, t, Np, D)\n        )\n\n        # üîÑ TEMPORAL: (b, Np, t+1, d)\n        self.temporal_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.num_frames + 1, self.dim))\n        self.temporal_token = nn.Parameter(torch.randn(1, 1, 1, self.dim))  # (1, 1, 1, d)\n        self.temporal_transformer = Transformer(self.dim, self.depth_temporal, heads, dim_head, dim * scale_dim, dropout)\n\n        # üîÑ SPATIAL\n        self.spatial_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n        self.space_transformer = Transformer(self.dim, self.depth_spatial, heads, dim_head, dim * scale_dim, dropout)\n\n        self.dropout = nn.Dropout(emb_dropout)\n        self.pool = pool\n \n    def forward(self, x):\n        # üî∏ Shape d'entr√©e : (b, t, c, h, w) = (4, 4, 512, 32, 32)\n        # print('input of vivit :', x.shape)\n        # print('before patches embedding : ')\n        x = self.to_patch_embedding(x)  # (b, t, Np, d)\n        # print('after patch embed : ', x.shape)\n\n        b, t, n, d = x.shape  # t=4, n=256, d=512\n\n        # üîπ Transpose pour regrouper par patch : ‚Üí (b, n, t, d)\n        x = x.permute(0, 2, 1, 3)\n        # print('after  regroupe 4 frames par path : ',x.shape)\n\n        # üîπ Ajouter un prediction token par patch group : shape ‚Üí (b, n, t+1, d)\n        pred_token = self.temporal_token.expand(b, n, 1, d)\n        x = torch.cat((pred_token, x), dim=2)\n\n        # üîπ Ajouter position embedding temporel\n        x = x + self.temporal_pos_embedding[:, :, :x.shape[2], :]\n        # print('afeter adding position embedding : ',x.shape)\n        x = self.dropout(x)\n        # print ('after dropout')\n\n        # print('temporal transformer input : ', x.shape)  # (b, n, t+1, d)\n\n        # üîπ Appliquer transformer temporel par groupe (fusionner batch et Np)\n        x = rearrange(x, 'b n t d -> (b n) t d')\n        x = self.temporal_transformer(x)\n        x = rearrange(x, '(b n) t d -> b n t d', b=b)\n        # print('after temporal transformer : ',x.shape)\n        # üîπ R√©cup√©rer le token de pr√©diction (position 0)\n        x = x[:, :, 0, :]  # (b, n, d)\n        # print('after temporal transformer : ',x.shape)\n        # print('start embedding spatial')\n        # üîπ Ajouter embedding spatial\n        x = x + self.spatial_pos_embedding\n        # print('spatial transformer input : ', x.shape)\n\n        # üîπ Transformer spatial\n        x = self.space_transformer(x)\n        # print('after spatial transformer : ',x.shape)\n        # print(self.image_size)\n\n        # üîπ Reshape final en feature map (b, c, h, w)\n        # x = rearrange(x, 'b (h w) c -> b c h w', h=self.image_size // self.patch_size)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=self.image_size//2,w=self.image_size//2)\n\n        # print ('after reshaping : ',x.shape)\n\n\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\n# class FlowNet(nn.Module):\n#     \"\"\"Module simplifi√© pour calculer le flux optique entre deux frames\"\"\"\n#     def __init__(self):\n#         super(FlowNet, self).__init__()\n        \n#         # Architecture simple pour estimer le flux optique\n#         self.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3)\n#         self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n#         self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n#         self.conv4 = nn.Conv2d(256, 2, kernel_size=3, stride=1, padding=1)\n        \n#     def forward(self, frame1, frame2):\n#         # Concat√©ner les deux frames le long de la dimension des canaux\n#         x = torch.cat([frame1, frame2], dim=1)\n        \n#         # Passer √† travers les couches\n#         x = F.relu(self.conv1(x))\n#         x = F.relu(self.conv2(x))\n#         x = F.relu(self.conv3(x))\n#         flow = self.conv4(x)\n        \n#         # Redimensionner le flux √† la taille originale\n#         flow = F.interpolate(flow, scale_factor=4, mode='bilinear', align_corners=True)\n        \n#         return flow\n\nclass ImprovedFlowNet(nn.Module):\n    \"\"\"Module am√©lior√© pour calculer le flux optique\"\"\"\n    def __init__(self):\n        super(ImprovedFlowNet, self).__init__()\n        \n        # Premi√®re partie - extraction de features\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n        )\n        \n        # Deuxi√®me partie - estimation du flux\n        self.flow_estimator = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),\n        )\n        \n        # Module de raffinement\n        self.refinement = nn.Sequential(\n            nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(16, 2, kernel_size=3, stride=1, padding=1),\n        )\n        \n    def forward(self, frame1, frame2):\n        # Concat√©nation des frames\n        x = torch.cat([frame1, frame2], dim=1)\n        \n        # Extraction des features\n        features = self.feature_extractor(x)\n        \n        # Estimation du flux\n        flow = self.flow_estimator(features)\n        \n        # Raffinement du flux\n        flow = self.refinement(flow)\n        \n        # Upsampling pour correspondre √† la taille d'entr√©e\n        flow = F.interpolate(flow, scale_factor=4, mode='bilinear', align_corners=True)\n        \n        return flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransAnomaly(nn.Module):\n    def __init__(self, batch_size=4, num_frames=4):\n        super(TransAnomaly, self).__init__()\n        self.batch_size = batch_size\n        self.num_frames = num_frames\n        self.channels_1 = 64\n        self.channels_2 = 128\n        self.channels_3 = 256\n        self.channels_4 = 512\n        \n        # Encoder\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=self.channels_1)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=self.channels_1, out_channels=self.channels_2)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=self.channels_2, out_channels=self.channels_3)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=self.channels_3, out_channels=self.channels_4)\n\n        # Residual connections\n        self.residual_14 = nn.Conv2d(in_channels=self.channels_1*self.num_frames, out_channels=self.channels_1, kernel_size=3, stride=1, padding=1)\n        self.residual_23 = nn.Conv2d(in_channels=self.channels_2*self.num_frames, out_channels=self.channels_2, kernel_size=3, stride=1, padding=1)\n        self.residual_32 = nn.Conv2d(in_channels=self.channels_3*self.num_frames, out_channels=self.channels_3, kernel_size=3, stride=1, padding=1)\n        self.residual_41 = nn.Conv2d(in_channels=self.channels_4*self.num_frames, out_channels=self.channels_4, kernel_size=3, stride=1, padding=1)\n\n        # ViViT layer\n        self.middle = ViViT(image_size=32, patch_size=2, num_frames=self.num_frames, in_channels=512)\n\n        # Decoder\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=self.channels_4, out_channels=self.channels_4, kernel_size=3, stride=2, padding=1, output_padding=1) \n        self.expansive_12 = self.conv_block(in_channels=self.channels_4*2, out_channels=self.channels_4)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=self.channels_4, out_channels=self.channels_3, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=self.channels_3*2, out_channels=self.channels_3)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=self.channels_3, out_channels=self.channels_2, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=self.channels_2*2, out_channels=self.channels_2)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=self.channels_2, out_channels=self.channels_1, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=self.channels_1*2, out_channels=self.channels_1)\n        self.output = nn.Conv2d(in_channels=self.channels_1, out_channels=3, kernel_size=3, stride=1, padding=1)\n        \n        # Module de flux optique\n        # self.flownet = FlowNet()\n        self.flownet = ImprovedFlowNet()\n\n        \n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels)\n        )\n        return block\n    \n    def compute_optical_flow_loss(self, pred_frame, target_frame, prev_frame):\n        \"\"\"\n        Calcule la perte de flux optique entre:\n        1. Le flux entre la pr√©diction et la frame pr√©c√©dente\n        2. Le flux entre la cible et la frame pr√©c√©dente\n        \"\"\"\n        # Calculer les deux flux optiques\n        flow_pred = self.flownet(prev_frame, pred_frame)\n        flow_target = self.flownet(prev_frame, target_frame)\n        \n        # Calculer la diff√©rence L1 entre les deux flux\n        loss = F.l1_loss(flow_pred, flow_target)\n        \n        return loss\n    \n    def forward(self, frames):\n        # frames.shape = (batch, num_frames, c, h, w)\n        batch_size = frames.shape[0]\n        \n        ########## Encoding #######\n        tmp_frames = rearrange(frames, 'b t c h w -> (b t) c h w')\n        \n        contracting_11_out = self.contracting_11(tmp_frames)\n        contracting_12_out = self.contracting_12(contracting_11_out)\n        contracting_21_out = self.contracting_21(contracting_12_out)\n        contracting_22_out = self.contracting_22(contracting_21_out)\n        contracting_31_out = self.contracting_31(contracting_22_out)\n        contracting_32_out = self.contracting_32(contracting_31_out)\n        contracting_41_out = self.contracting_41(contracting_32_out)\n        \n        ####### ViViT layer ########\n        vivit_input = rearrange(contracting_41_out, '(b t) c h w -> b t c h w', b=batch_size)\n        middle_out = self.middle(vivit_input)\n        \n        ######### Residual connections #####\n        residual_14_out = rearrange(contracting_11_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_14_out = self.residual_14(residual_14_out)\n\n        residual_23_out = rearrange(contracting_21_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_23_out = self.residual_23(residual_23_out)\n\n        residual_32_out = rearrange(contracting_31_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_32_out = self.residual_32(residual_32_out)\n\n        residual_41_out = rearrange(contracting_41_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_41_out = self.residual_41(residual_41_out)\n        \n        ####### Decoding ##########\n        expansive_11_out = self.expansive_11(middle_out)\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, residual_41_out), dim=1))\n        expansive_21_out = self.expansive_21(expansive_12_out)\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, residual_32_out), dim=1))\n        expansive_31_out = self.expansive_31(expansive_22_out)\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, residual_23_out), dim=1))\n        expansive_41_out = self.expansive_41(expansive_32_out)\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, residual_14_out), dim=1))\n        output_out = self.output(expansive_42_out)\n        \n        return output_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## les fonction de loss","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class GradientLoss(nn.Module):\n#     \"\"\"Perte de gradient pour am√©liorer la nettet√© des images pr√©dites\"\"\"\n#     def __init__(self):\n#         super(GradientLoss, self).__init__()\n        \n#     def forward(self, pred_frame, target_frame):\n#         \"\"\"\n#         Calcule la diff√©rence entre les gradients des images pr√©dites et cibles\n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image pr√©dite\n#             target_frame: tensor [B, C, H, W] - image cible\n#         Returns:\n#             loss: tensor scalaire - perte de gradient\n#         \"\"\"\n#         # Calcul des gradients horizontaux et verticaux pour l'image pr√©dite\n#         pred_dx = pred_frame[:, :, :, :-1] - pred_frame[:, :, :, 1:]  # Gradient horizontal\n#         pred_dy = pred_frame[:, :, :-1, :] - pred_frame[:, :, 1:, :]  # Gradient vertical\n        \n#         # Calcul des gradients horizontaux et verticaux pour l'image cible\n#         target_dx = target_frame[:, :, :, :-1] - target_frame[:, :, :, 1:]\n#         target_dy = target_frame[:, :, :-1, :] - target_frame[:, :, 1:, :]\n        \n#         # Calcul des pertes L1 pour les gradients\n#         loss_x = F.l1_loss(pred_dx, target_dx)\n#         loss_y = F.l1_loss(pred_dy, target_dy)\n        \n#         return loss_x + loss_y\n\n# class OpticalFlowLoss(nn.Module):\n#     \"\"\"Perte de flux optique pour la coh√©rence temporelle\"\"\"\n#     def __init__(self):\n#         super(OpticalFlowLoss, self).__init__()\n        \n#     def forward(self, pred_frame, target_frame, prev_frame, flownet):\n#         \"\"\"\n#         Calcule la perte de flux optique entre:\n#         1. Le flux entre la pr√©diction et la frame pr√©c√©dente\n#         2. Le flux entre la cible et la frame pr√©c√©dente\n        \n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image pr√©dite\n#             target_frame: tensor [B, C, H, W] - image cible\n#             prev_frame: tensor [B, C, H, W] - frame pr√©c√©dente\n#             flownet: nn.Module - r√©seau pour calculer le flux optique\n#         Returns:\n#             loss: tensor scalaire - perte de flux optique\n#         \"\"\"\n#         # Calculer les deux flux optiques\n#         flow_pred = flownet(prev_frame, pred_frame)\n#         flow_target = flownet(prev_frame, target_frame)\n        \n#         # Calculer la diff√©rence L1 entre les deux flux\n#         loss = F.l1_loss(flow_pred, flow_target)\n        \n#         return loss\n\n# class CombinedLoss(nn.Module):\n#     \"\"\"Combinaison des diff√©rentes pertes avec pond√©ration\"\"\"\n#     def __init__(self, identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05):\n#         \"\"\"\n#         Args:\n#             identity_weight: float - poids pour la perte d'identit√© (MSE)\n#             gradient_weight: float - poids pour la perte de gradient\n#             flow_weight: float - poids pour la perte de flux optique\n#         \"\"\"\n#         super(CombinedLoss, self).__init__()\n#         self.identity_loss = nn.MSELoss()\n#         self.gradient_loss = GradientLoss()\n#         self.optical_flow_loss = OpticalFlowLoss()\n#         self.identity_weight = identity_weight\n#         self.gradient_weight = gradient_weight\n#         self.flow_weight = flow_weight\n        \n#     def forward(self, pred_frame, target_frame, prev_frame, flownet):\n#         \"\"\"\n#         Calcule la perte combin√©e\n        \n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image pr√©dite\n#             target_frame: tensor [B, C, H, W] - image cible\n#             prev_frame: tensor [B, C, H, W] - frame pr√©c√©dente\n#             flownet: nn.Module - r√©seau pour calculer le flux optique\n#         Returns:\n#             loss: tensor scalaire - perte combin√©e pond√©r√©e\n#         \"\"\"\n#         # Perte d'identit√© (MSE)\n#         identity = self.identity_loss(pred_frame, target_frame) * self.identity_weight\n        \n#         # Perte de gradient\n#         gradient = self.gradient_loss(pred_frame, target_frame) * self.gradient_weight\n        \n#         # Perte de flux optique\n#         flow = self.optical_flow_loss(pred_frame, target_frame, prev_frame, flownet) * self.flow_weight\n        \n#         return identity + gradient + flow\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GradientLoss(nn.Module):\n    \"\"\"Perte de gradient am√©lior√©e avec gestion des bords\"\"\"\n    def __init__(self, epsilon=1e-6):\n        super(GradientLoss, self).__init__()\n        self.epsilon = epsilon\n        \n        # Initialisation des noyaux de convolution une seule fois\n        self.register_buffer('kernel_x', torch.tensor([[[[1.0, -1.0]]]]))\n        self.register_buffer('kernel_y', torch.tensor([[[[1.0], [-1.0]]]]))\n        \n    def forward(self, pred_frame, target_frame):\n        \"\"\"\n        Version corrig√©e avec :\n        - Conversion automatique des types\n        - Gestion des bords optimis√©e\n        \"\"\"\n        # V√©rification des dimensions\n        if pred_frame.dim() != 4 or target_frame.dim() != 4:\n            raise ValueError(\"Les inputs doivent √™tre de dimension [B, C, H, W]\")\n            \n        # Calcul des gradients\n        pred_dx = F.conv2d(pred_frame, \n                          self.kernel_x.repeat(pred_frame.size(1), 1, 1, 1),\n                          padding=0, \n                          groups=pred_frame.size(1))\n        \n        pred_dy = F.conv2d(pred_frame, \n                          self.kernel_y.repeat(pred_frame.size(1), 1, 1, 1),\n                          padding=0, \n                          groups=pred_frame.size(1))\n        \n        target_dx = F.conv2d(target_frame, \n                            self.kernel_x.repeat(target_frame.size(1), 1, 1, 1),\n                            padding=0, \n                            groups=target_frame.size(1))\n        \n        target_dy = F.conv2d(target_frame, \n                            self.kernel_y.repeat(target_frame.size(1), 1, 1, 1),\n                            padding=0, \n                            groups=target_frame.size(1))\n        \n        # Calcul des pertes L1\n        loss_x = F.l1_loss(pred_dx, target_dx)\n        loss_y = F.l1_loss(pred_dy, target_dy)\n        \n        return (loss_x + loss_y) / 2\n\nclass OpticalFlowLoss(nn.Module):\n    \"\"\"Perte de flux optique simplifi√©e mais efficace\"\"\"\n    def __init__(self):\n        super(OpticalFlowLoss, self).__init__()\n        \n    def forward(self, pred_frame, target_frame, prev_frame, flownet):\n        # Calcul des flux\n        flow_pred = flownet(prev_frame, pred_frame)\n        flow_target = flownet(prev_frame, target_frame)\n        \n        # Perte L1 simple\n        return F.l1_loss(flow_pred, flow_target)\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Version simplifi√©e mais robuste de CombinedLoss\"\"\"\n    def __init__(self, identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05):\n        super(CombinedLoss, self).__init__()\n        self.identity_loss = nn.MSELoss()\n        self.gradient_loss = GradientLoss()\n        self.optical_flow_loss = OpticalFlowLoss()\n        \n        # Poids fixes (version simplifi√©e)\n        self.identity_weight = identity_weight\n        self.gradient_weight = gradient_weight\n        self.flow_weight = flow_weight\n        \n    def forward(self, pred_frame, target_frame, prev_frame, flownet):\n        # Calcul des pertes\n        identity = self.identity_loss(pred_frame, target_frame) * self.identity_weight\n        gradient = self.gradient_loss(pred_frame, target_frame) * self.gradient_weight\n        flow = self.optical_flow_loss(pred_frame, target_frame, prev_frame, flownet) * self.flow_weight\n        \n        return identity + gradient + flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Entrainement","metadata":{}},{"cell_type":"code","source":"# Fonction d'accuracy et d'entra√Ænement\ndef pixel_accuracy(preds, targets):\n    preds = torch.sigmoid(preds)\n    preds = (preds > 0.5).float()\n    targets = (targets > 0.5).float()\n    correct = (preds == targets).float()\n    return correct.sum() / correct.numel()\n\ndef train(model, dataloader, optimizer, epoch, log_interval=200):\n    model.train()\n    total_loss, total_acc = 0.0, 0.0\n    \n    for batch_idx, (input_sequences, target_frames, _) in enumerate(dataloader):\n        input_sequences = input_sequences.to(device)\n        target_frames = target_frames.to(device)\n        \n        # La derni√®re frame d'entr√©e est la frame pr√©c√©dente\n        prev_frames = input_sequences[:, -1, :, :, :]\n        \n        optimizer.zero_grad()\n        predicted_frames = model(input_sequences)\n        \n        # Utilisez model.flownet au lieu de flownet\n        loss = criterion(predicted_frames, target_frames, prev_frames, model.flownet)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_acc += pixel_accuracy(predicted_frames, target_frames).item()\n        \n        if batch_idx % log_interval == 0:\n            avg_loss = total_loss / (batch_idx + 1)\n            avg_acc = total_acc / (batch_idx + 1)\n            print(f\"Epoch {epoch} | Batch {batch_idx}/{len(dataloader)} | \"\n                  f\"Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}\")\n\n    avg_loss = total_loss / len(dataloader)\n    avg_acc = total_acc / len(dataloader)\n    print(f\"\\nEpoch {epoch} Summary: Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.4f}\\n\")\n    return avg_loss, avg_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n\n# Initialisation du mod√®le\nmodel = TransAnomaly().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\ncriterion = CombinedLoss(identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05).to(device)\n\n# Boucle d'entra√Ænement\nlosses, accuracies = [], []\nEPOCHS = 6\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_loss, epoch_acc = train(model, train_dataloader, optimizer, epoch)\n    losses.append(epoch_loss)\n    accuracies.append(epoch_acc)\n    print(f\"[EPOCH: {epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n\n# ‚ûï Plotting\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.plot(losses, label='Loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss par Epoch\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(accuracies, label='Accuracy', color='green')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy par Epoch\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation avec ROC courb","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns  # Pour une meilleure visualisation de la matrice de confusion\n\ndef calculate_psnr(img1, img2, max_pixel=1.0):\n    \"\"\"Calcule le PSNR entre deux images (suppos√©es normalis√©es entre -1 et 1).\"\"\"\n    img1 = (img1 + 1) / 2  # Conversion [-1, 1] -> [0, 1]\n    img2 = (img2 + 1) / 2\n    mse = torch.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 10 * torch.log10(max_pixel ** 2 / mse).item()\n\ndef evaluate_model(model, dataloader, device, window_size=64, stride=32):\n    model.eval()\n    psnrs = []\n    \n    with torch.no_grad():\n        for input_frames, target_frames, _ in dataloader:\n            input_frames = input_frames.to(device)\n            target_frames = target_frames.to(device)\n            predicted_frames = model(input_frames)\n            for i in range(predicted_frames.shape[0]):\n                psnr = calculate_psnr(predicted_frames[i], target_frames[i])\n                psnrs.append(psnr)\n    \n    psnrs = np.array(psnrs)\n    scores = (psnrs - psnrs.min()) / (psnrs.max() - psnrs.min())\n    fake_labels = np.zeros_like(scores)\n    fake_labels[np.argsort(scores)[:len(scores)//10]] = 1  # 10% lowest PSNR = anomalies\n    \n    fpr, tpr, thresholds = roc_curve(fake_labels, 1 - scores)  # Inverser car PSNR bas = anomalie\n    roc_auc = auc(fpr, tpr)\n    optimal_idx = np.argmax(tpr - fpr)\n    optimal_threshold = thresholds[optimal_idx]\n    predictions = (1 - scores) > optimal_threshold\n    tn, fp, fn, tp = confusion_matrix(fake_labels, predictions).ravel()\n    \n    return roc_auc, (tp, fp, tn, fn), scores, fpr, tpr, confusion_matrix(fake_labels, predictions)  # Ajout de la matrice de confusion compl√®te\n\n# Utilisation\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# √âvaluation et calcul de l'AUC\nroc_auc, (tp, fp, tn, fn), scores, fpr, tpr, conf_matrix = evaluate_model(model, test_dataloader, device)\n\n# Affichage des r√©sultats\nprint(f\"[AUC] = {roc_auc:.4f}\")\nprint(f\"Matrice de confusion: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\nprint(f\"Precision = {tp / (tp + fp):.4f}, Recall = {tp / (tp + fn):.4f}\")\n\n# Trac√© de la courbe ROC\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\n# Trac√© de la matrice de confusion\nplt.subplot(1, 2, 2)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Normal', 'Anomaly'], \n            yticklabels=['Normal', 'Anomaly'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation pour un seul video","metadata":{}},{"cell_type":"code","source":"## Extraire les frames d'un seul video","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\n\n# Chemin de la vid√©o √† traiter\nvideo_path = \"/kaggle/input/anomalous-action-detection-dataset/Anomalous Action Detection Dataset( Ano-AAD)/abnormal class/Fighting/46.mp4\"\noutput_folder=\"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale video\"\n\n# S'assurer que le dossier de sortie existe\nos.makedirs(output_folder, exist_ok=True)\n\n# Extraire le nom de la vid√©o sans extension\nvideo_name = os.path.splitext(os.path.basename(video_path))[0]\nvideo_output_folder = os.path.join(output_folder, video_name)\nos.makedirs(video_output_folder, exist_ok=True)\n\n# Ouvrir la vid√©o\ncap = cv2.VideoCapture(video_path)\n    \n# V√©rifier si la vid√©o s'ouvre correctement\nif not cap.isOpened():\n    print(f\"‚ùå Erreur de chargement : {video_path}\")\nelse:\n    print(f\"üé• Traitement de {video_path}\")\n\n    # Extraire et sauvegarder chaque frame\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # Fin de la vid√©o\n\n        # Sauvegarder l'image en JPG\n        frame_filename = os.path.join(video_output_folder, f\"frame_{frame_idx:04d}.jpg\")\n        cv2.imwrite(frame_filename, frame)\n        frame_idx += 1\n\n    cap.release()\n    print(f\"‚úî {frame_idx} Frames sauvegard√©es dans : {video_output_folder}\")\n    print(\"‚úÖ Extraction termin√©e !\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## pour cadrer le nombre de frames  utiliser pour chaque video","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Dossier source de la vid√©o (images/frames)\ninput_video_folder = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale video/46\"  # <-- Remplace par le nom du dossier de ta vid√©o\noutput_folder = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale videok\"\n\n\n# Nombre maximum de frames √† copier\nMAX_FRAMES = 450\n\n# S'assurer que le dossier de sortie existe\nos.makedirs(output_folder, exist_ok=True)\n\n# Lister et trier les frames\nframes = sorted(os.listdir(input_video_folder))\n\nframes_to_copy = frames[:MAX_FRAMES]\n\n# Copier les frames\nfor frame in frames_to_copy:\n    src = os.path.join(input_video_folder, frame)\n    dst = os.path.join(output_folder, frame)\n    shutil.copy(src, dst)\n\nprint(f\"‚úÖ {len(frames_to_copy)} frames copi√©es depuis {input_video_folder} vers {output_folder}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DataLoader pour un seule video :","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass SingleVideoDataset(Dataset):\n    def __init__(self, frames_dir, sequence_length=4, transform=None, overlap=True):\n        self.frames_dir = frames_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n\n        # Charger et trier les frames\n        self.frames = sorted(\n            [f for f in os.listdir(frames_dir) if f.startswith('frame_')],\n            key=lambda x: int(x.split('_')[1].split('.')[0]))\n        \n        # Calculer le nombre maximal de s√©quences compl√®tes\n        max_sequences = len(self.frames) - sequence_length\n        if max_sequences <= 0:\n            raise ValueError(\"Pas assez de frames pour former une s√©quence !\")\n        \n        # G√©n√©rer les s√©quences avec troncature finale\n        if overlap:\n            self.sequences = [\n                [os.path.join(frames_dir, self.frames[j]) \n                 for j in range(i, i + sequence_length + 1)]\n                for i in range(max_sequences)\n            ]\n        else:\n            step = sequence_length + 1\n            self.sequences = [\n                [os.path.join(frames_dir, self.frames[j]) \n                 for j in range(i, i + sequence_length + 1)]\n                for i in range(0, max_sequences, step)\n            ]\n        \n        # Tronquer pour avoir un nombre de s√©quences divisible par le batch_size\n        self.total_sequences = len(self.sequences)\n\n    def __len__(self):\n        return self.total_sequences\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        \n        for path in frame_paths:\n            img = cv2.imread(path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            \n            if self.transform:\n                img = self.transform(img)\n                \n            images.append(img)\n\n        input_frames = torch.stack(images[:-1], dim=0)  # Les N premi√®res frames\n        target_frame = images[-1]  # La derni√®re frame\n        return input_frames, target_frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# D√©finir les transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Cr√©ation du Dataset pour une seule vid√©o\nvideo_frames_dir = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale videok\" \ndataset = SingleVideoDataset(\n    frames_dir=video_frames_dir,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\n# Cr√©ation du DataLoader\nsignal_dataloader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=False,  # Garder l'ordre temporel\n    num_workers=2,\n    pin_memory=True,\n    drop_last=True  # <-- Ajoutez cette ligne\n\n)\n\n# # Exemple d'utilisation\n# for inputs, targets in signal_dataloader:\n#     print(f\"Batch input shape: {inputs.shape}\")  # [4, 4, 3, 256, 256]\n#     print(f\"Batch target shape: {targets.shape}\")  # [4, 3, 256, 256]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation d'un seul video long ou plusieurs video moyen ( le graphe de score d'anomalie au cours de temps)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n\ndef calculate_psnr(pred, target, max_val=1.0):\n    \"\"\"Calcule le PSNR entre une frame pr√©dite et la frame r√©elle.\"\"\"\n    pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n    target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n    return psnr(target_np, pred_np, data_range=max_val)\n\ndef sliding_window_psnr(pred, target, window_size=64, stride=32, max_val=1.0):\n    \"\"\"Calcule le PSNR sur les patches avec les plus grandes erreurs (sliding window).\"\"\"\n    h, w = pred.shape[-2], pred.shape[-1]\n    mse_patches = []\n    \n    for i in range(0, h - window_size + 1, stride):\n        for j in range(0, w - window_size + 1, stride):\n            patch_pred = pred[..., i:i+window_size, j:j+window_size]\n            patch_target = target[..., i:i+window_size, j:j+window_size]\n            mse = torch.mean((patch_pred - patch_target) ** 2).item()\n            mse_patches.append(mse)\n    \n    if not mse_patches:\n        return 0.0\n    \n    # Prend les p patches avec les plus grandes MSE (p = moiti√©)\n    p = len(mse_patches) // 2\n    top_mse = sorted(mse_patches, reverse=True)[:p]\n    avg_mse = np.mean(top_mse)\n    return 10 * np.log10(max_val ** 2 / avg_mse) if avg_mse > 0 else 100.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# without opticale flow\n\n# def evaluate_anomaly_unlabeled(model, dataloader, device, use_sliding_window=True, window_size=64, stride=32):\n#     model.eval()\n#     psnr_scores = []\n#     frame_predictions = []\n\n#     with torch.no_grad():\n#         for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):  # Ajoutez _ pour ignorer le 3√®me √©l√©ment\n#             input_sequences = input_sequences.to(device)\n#             target_frames = target_frames.to(device)\n#             predicted_frames = model(input_sequences)\n            \n#             for pred, target in zip(predicted_frames, target_frames):\n#                 if use_sliding_window:\n#                     psnr_score = sliding_window_psnr(pred, target, window_size, stride)\n#                 else:\n#                     psnr_score = calculate_psnr(pred, target)\n#                 psnr_scores.append(psnr_score)\n#                 frame_predictions.append((pred.cpu(), target.cpu()))\n\n#     min_psnr, max_psnr = min(psnr_scores), max(psnr_scores)\n#     regularity_scores = [(psnr - min_psnr) / (max_psnr - min_psnr + 1e-6) for psnr in psnr_scores]\n    \n#     return regularity_scores, frame_predictions\n\n#with opticale flow\n\ndef evaluate_anomaly_unlabeled(model, dataloader, device, use_sliding_window=True, window_size=64, stride=32):\n    model.eval()\n    psnr_scores = []\n    frame_predictions = []\n\n    with torch.no_grad():\n        for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n            input_sequences = input_sequences.to(device)\n            target_frames = target_frames.to(device)\n            \n            # La derni√®re frame d'entr√©e est la frame pr√©c√©dente\n            prev_frames = input_sequences[:, -1, :, :, :]\n            \n            predicted_frames = model(input_sequences)\n            \n            # Calcul du flux optique entre la frame pr√©c√©dente et la pr√©diction\n            flow_pred = model.flownet(prev_frames, predicted_frames)\n            \n            for pred, target, flow in zip(predicted_frames, target_frames, flow_pred):\n                if use_sliding_window:\n                    psnr_score = sliding_window_psnr(pred, target, window_size, stride)\n                else:\n                    psnr_score = calculate_psnr(pred, target)\n                psnr_scores.append(psnr_score)\n                frame_predictions.append((pred.cpu(), target.cpu(), flow.cpu()))\n\n    min_psnr, max_psnr = min(psnr_scores), max(psnr_scores)\n    regularity_scores = [(psnr - min_psnr) / (max_psnr - min_psnr + 1e-6) for psnr in psnr_scores]\n    \n    return regularity_scores, frame_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_anomalies(scores, threshold, num_frames_to_plot=5):\n    anomalous_indices = np.where(np.array(scores) < threshold)[0]\n    plt.figure(figsize=(15, 5))\n    plt.plot(scores, label='Scores de r√©gularit√©')\n    plt.axhline(y=threshold, color='r', linestyle='--', label='Seuil d\\'anomalie')\n    plt.scatter(anomalous_indices[:num_frames_to_plot], \n                [scores[i] for i in anomalous_indices[:num_frames_to_plot]], \n                color='red', label='Frames anormales')\n    plt.legend()\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"Score (0=anomalie, 1=normal)\")\n    plt.title(\"D√©tection d'anomalies\")\n    plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## apparence d'anomalie a l'aide de optical flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\n\n# 1. Scores\nscores, frames = evaluate_anomaly_unlabeled(model, signal_dataloader, device)\n\n# 2. Seuil\nthreshold = np.percentile(scores, 5)\n\n# 3. Visualisation\nplot_anomalies(scores, threshold)\n\n# 4. Indices anormaux\nanomalous_indices = np.where(np.array(scores) < threshold)[0]\n\n\n# 5. Affichage des 10 premi√®res anomalies\nfor idx in anomalous_indices[:30]:\n    pred, target, _ = frames[idx]\n\n    def prepare_image(tensor):\n        img = tensor.numpy()\n        if len(img.shape) == 3 and img.shape[0] == 3:\n            img = img.transpose(1, 2, 0)  # (C, H, W) ‚Üí (H, W, C)\n        img = (img - img.min()) / (img.max() - img.min() + 1e-6)  # Normalisation [0, 1]\n        return np.clip(img, 0, 1)\n\n    # --- Calcul du masque d'erreur ---\n    diff = torch.abs(pred - target)\n    error_map = diff.mean(dim=0)  # Moyenne sur les canaux RGB\n    norm_error = (error_map - error_map.min()) / (error_map.max() - error_map.min() + 1e-6)\n    blurred_mask = cv2.GaussianBlur(norm_error.numpy(), (31, 31), 10)  # R√©duction du flou pour plus de d√©tails\n\n    # --- Cr√©ation de la heatmap am√©lior√©e ---\n    heatmap = cv2.applyColorMap((blurred_mask * 255).astype(np.uint8), cv2.COLORMAP_INFERNO)  # Colormap plus contrast√©e\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n\n    # Renforcement des couleurs (HSV)\n    hsv = cv2.cvtColor(heatmap, cv2.COLOR_RGB2HSV)\n    hsv[..., 1] = hsv[..., 1] * 1.8  # Saturation augment√©e\n    hsv[..., 2] = np.clip(hsv[..., 2] * 1.3, 0, 1)  # Luminosit√© augment√©e\n    heatmap = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\n    # --- Transparence adaptative ---\n    alpha_mask = np.clip(blurred_mask * 3.5, 0, 1)  # R√©duction de la transparence\n    base = np.ones_like(heatmap) * 0.9  # Fond l√©g√®rement gris pour mieux contraster\n\n    # Fusion finale\n    transparent_heatmap = heatmap * alpha_mask[..., np.newaxis] + base * (1 - alpha_mask[..., np.newaxis])\n\n    # --- Affichage ---\n    plt.figure(figsize=(18, 6), facecolor='white')\n\n    # Image r√©elle\n    plt.subplot(1, 3, 1)\n    plt.imshow(prepare_image(target))\n    plt.title(\"Image R√©elle\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    # Image pr√©dite\n    plt.subplot(1, 3, 2)\n    plt.imshow(prepare_image(pred))\n    plt.title(f\"Pr√©diction (score: {scores[idx]:.2f})\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    # Heatmap des anomalies\n    plt.subplot(1, 3, 3)\n    plt.imshow(transparent_heatmap)\n    plt.title(\"Flux Optique - Anomalies\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation avec bounding box    ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\n\ndef sliding_window_anomaly_detection(pred, target, window_size=64, stride=32, threshold=0.1):\n    \"\"\"D√©tecte les patches anormaux et retourne leurs coordonn√©es\"\"\"\n    h, w = pred.shape[-2], pred.shape[-1]\n    anomaly_boxes = []\n    \n    for i in range(0, h - window_size + 1, stride):\n        for j in range(0, w - window_size + 1, stride):\n            patch_pred = pred[..., i:i+window_size, j:j+window_size]\n            patch_target = target[..., i:i+window_size, j:j+window_size]\n            \n            # Calcul de l'erreur MSE pour le patch\n            mse = torch.mean((patch_pred - patch_target) ** 2).item()\n            \n            # Si l'erreur d√©passe le seuil, on consid√®re le patch comme anormal\n            if mse > threshold:\n                anomaly_boxes.append((j, i, j+window_size, i+window_size, mse))\n    \n    return anomaly_boxes\n\n# def evaluate_and_visualize_anomalies(model, dataloader, device, window_size=64, stride=32, anomaly_threshold=0.1, display_count=5):\n#     model.eval()\n    \n#     with torch.no_grad():\n#         for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n#             input_sequences = input_sequences.to(device)\n#             target_frames = target_frames.to(device)\n#             predicted_frames = model(input_sequences)\n            \n#             for idx, (pred, target) in enumerate(zip(predicted_frames, target_frames)):\n#                 # Convertir les tenseurs en images numpy\n#                 pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)\n#                 target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n                \n#                 # Normalisation des images\n#                 pred_np = (pred_np - pred_np.min()) / (pred_np.max() - pred_np.min() + 1e-6)\n#                 target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-6)\n                \n#                 # D√©tection des anomalies par sliding window\n#                 anomaly_boxes = sliding_window_anomaly_detection(pred, target, window_size, stride, anomaly_threshold)\n                \n#                 # Visualisation\n#                 if len(anomaly_boxes) > 0 and idx < display_count:\n#                     plt.figure(figsize=(15, 6))\n                    \n#                     # Afficher l'image originale avec les bounding boxes\n#                     plt.subplot(1, 2, 1)\n#                     plt.imshow(target_np)\n#                     plt.title(\"Image Originale avec Anomalies D√©tect√©es\")\n                    \n#                     # Dessiner les bounding boxes autour des anomalies\n#                     for box in anomaly_boxes:\n#                         x1, y1, x2, y2, mse = box\n#                         rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n#                                                linewidth=2, edgecolor='r', facecolor='none')\n#                         plt.gca().add_patch(rect)\n#                         plt.text(x1, y1, f\"{mse:.2f}\", color='white', \n#                                 bbox=dict(facecolor='red', alpha=0.5))\n                    \n#                     # Afficher l'image pr√©dite pour comparaison\n#                     plt.subplot(1, 2, 2)\n#                     plt.imshow(pred_np)\n#                     plt.title(\"Image Pr√©dite\")\n                    \n#                     plt.tight_layout()\n#                     plt.show()\n\n# # Param√®tres\n# window_size = 64  # Taille des patches\n# stride = 32       # Pas du sliding window\n# anomaly_threshold = 0.1  # Seuil d'anomalie (√† ajuster selon votre cas)\n\n# # Ex√©cution\n# evaluate_and_visualize_anomalies(model, signal_dataloader, device, \n#                                 window_size=window_size, \n#                                 stride=stride, \n#                                 anomaly_threshold=anomaly_threshold)\n\n\ndef evaluate_and_visualize_anomalies(model, dataloader, device, window_size=64, stride=32, \n                                   anomaly_threshold=0.1, max_display=20):  # Ajout du param√®tre max_display\n    model.eval()\n    displayed_count = 0  # Compteur pour les frames affich√©es\n    \n    with torch.no_grad():\n        for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n            if displayed_count >= max_display:  # Arr√™ter apr√®s 20 frames\n                break\n                \n            input_sequences = input_sequences.to(device)\n            target_frames = target_frames.to(device)\n            predicted_frames = model(input_sequences)\n            \n            for idx, (pred, target) in enumerate(zip(predicted_frames, target_frames)):\n                if displayed_count >= max_display:  # V√©rification √† chaque frame\n                    break\n                    \n                # Convertir les tenseurs en images numpy\n                pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)\n                target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n                \n                # Normalisation des images\n                pred_np = (pred_np - pred_np.min()) / (pred_np.max() - pred_np.min() + 1e-6)\n                target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-6)\n                \n                # D√©tection des anomalies par sliding window\n                anomaly_boxes = sliding_window_anomaly_detection(pred, target, window_size, stride, anomaly_threshold)\n                \n                # Visualisation seulement si anomalies d√©tect√©es\n                if len(anomaly_boxes) > 0:\n                    displayed_count += 1  # Incr√©menter le compteur\n                    \n                    plt.figure(figsize=(15, 6))\n                    \n                    # Afficher l'image originale avec les bounding boxes\n                    plt.subplot(1, 2, 1)\n                    plt.imshow(target_np)\n                    plt.title(f\"Frame Anormale {displayed_count}/{max_display}\")\n                    \n                    # Dessiner les bounding boxes autour des anomalies\n                    for box in anomaly_boxes:\n                        x1, y1, x2, y2, mse = box\n                        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n                                               linewidth=2, edgecolor='r', facecolor='none')\n                        plt.gca().add_patch(rect)\n                        plt.text(x1, y1, f\"{mse:.2f}\", color='white', \n                                bbox=dict(facecolor='red', alpha=0.5))\n                    \n                    # Afficher l'image pr√©dite pour comparaison\n                    plt.subplot(1, 2, 2)\n                    plt.imshow(pred_np)\n                    plt.title(\"Pr√©diction Correspondante\")\n                    \n                    plt.tight_layout()\n                    plt.show()\n\n# Param√®tres (inchang√©s)\nwindow_size = 64\nstride = 32\nanomaly_threshold = 0.1\n\n# Ex√©cution avec affichage des 20 premi√®res frames anormales\nevaluate_and_visualize_anomalies(model, signal_dataloader, device, \n                                window_size=window_size, \n                                stride=stride, \n                                anomaly_threshold=anomaly_threshold,\n                                max_display=20)  # Nouveau param√®tre","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}