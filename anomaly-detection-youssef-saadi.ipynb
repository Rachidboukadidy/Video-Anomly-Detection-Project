{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Clone the repository\n!git clone https://github.com/rishikksh20/ViViT-pytorch.git \n# Step 2: Change directory to the cloned repo\n%cd ViViT-pytorch\n\n# Step 4: Run scripts or modify files\n!python module.py vivit.py  # Example script to run\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## setup libraries & device","metadata":{}},{"cell_type":"code","source":"import os, sys\nimport natsort # For number sorting\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport natsort\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import einsum\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom module import Attention, PreNorm, FeedForward\nimport cv2\nfrom google.colab.patches import cv2_imshow # colab에서 cv2.imshow 사용 불가\nsys.path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\nprint(device)\nprint(torch.cuda.is_available())  # Should print True if GPU is enabled\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nprint(torch.cuda.device_count())  # Nombre de GPUs\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Pretraitement de dataloader pour surveillance camera","metadata":{}},{"cell_type":"code","source":"class DashcamDataset(Dataset):\n    def __init__(self, root_dir, sequence_length=4, transform=None, overlap=True):\n        self.root_dir = root_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n\n        # Parcourir chaque vidéo séparément\n        for video_folder in sorted(os.listdir(root_dir)):\n            video_path = os.path.join(root_dir, video_folder)\n            if os.path.isdir(video_path):\n                frames = sorted(\n                    [f for f in os.listdir(video_path) if f.startswith('frame_')],\n                    key=lambda x: int(x.split('_')[1].split('.')[0]))  # Trie par numéro (001 -> 1)\n                frames = [os.path.join(video_path, f) for f in frames]\n\n                # Générer les séquences pour cette vidéo\n                if overlap:\n                    for i in range(len(frames) - sequence_length):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n                else:\n                    for i in range(0, len(frames) - sequence_length, sequence_length + 1):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        for img_path in frame_paths:\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            if self.transform:\n                img = self.transform(img)\n            images.append(img)\n\n        input_frames = torch.stack(images[:self.sequence_length], dim=0)\n        target_frame = images[self.sequence_length]\n        target_name = os.path.basename(frame_paths[-1])\n        return input_frames, target_frame, target_name","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définition des transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalisation [-1, 1]\n])\n\n# Création du Dataset et du DataLoader avec la nouvelle classe améliorée\ntrain_dataset = DashcamDataset(\n    root_dir=\"/kaggle/input/surveillance-camera-fightnofight/Surveillance Camera Fight Dataset/Train\",\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=False,  # Garder False pour l'entraînement pour maintenir l'ordre temporel\n    num_workers=4,\n    pin_memory=True,\n)\n\ntest_dataset = DashcamDataset(\n    root_dir=\"/kaggle/input/surveillance-camera-fightnofight/Surveillance Camera Fight Dataset/Test\",\n    sequence_length=4,\n    transform=transform,\n    overlap=True  # Paramètre valide pour DashcamDataset\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,  # Désactivé pour maintenir l'ordre temporel\n    num_workers=4,\n    pin_memory=True,\n)\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef display_sequence_with_target(input_frames, target_frame, sequence_idx, target_name):\n    \"\"\"\n    Affiche les frames d'entrée et la frame cible avec des informations détaillées.\n    \"\"\"\n    # Convertir et préparer les images\n    input_frames = input_frames.cpu().numpy().transpose(0, 2, 3, 1)\n    target_frame = target_frame.cpu().numpy().transpose(1, 2, 0)\n    input_frames = (input_frames + 1) / 2\n    target_frame = (target_frame + 1) / 2\n\n    # Extraire les numéros de frame\n    target_num = target_name.split('_')[1].split('.')[0]\n    input_nums = [str(int(target_num) - len(input_frames) + i) for i in range(len(input_frames))]\n    \n    # Créer la figure\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(\n        f\"Sequence {sequence_idx + 1}\\n\"\n        f\"Input frames ({len(input_frames)}): {', '.join(input_nums)}\\n\"\n        f\"Target frame: {target_num}\",\n        fontsize=12, y=1.05\n    )\n    \n    # Afficher les frames d'entrée\n    for i in range(len(input_frames)):\n        plt.subplot(1, len(input_frames) + 1, i + 1)\n        plt.imshow(input_frames[i])\n        plt.title(f\"Frame {input_nums[i]}\")\n        plt.axis('off')\n    \n    # Afficher la cible\n    plt.subplot(1, len(input_frames) + 1, len(input_frames) + 1)\n    plt.imshow(target_frame)\n    plt.title(f\"Target: {target_num}\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Paramètre configurable - CHANGEZ ICI LE NOMBRE DE BATCHS À AFFICHER\nNUM_BATCHES_TO_DISPLAY = 3  # ← Modifiez ce nombre selon vos besoins\n\n# Itération sur le DataLoader\nfor batch_idx, (input_frames, target_frames, target_names) in enumerate(train_dataloader):\n    if batch_idx >= NUM_BATCHES_TO_DISPLAY:\n        break\n        \n    print(f\"\\n{'='*40}\")\n    print(f\"=== BATCH {batch_idx + 1}/{NUM_BATCHES_TO_DISPLAY} ===\")\n    print(f\"{'='*40}\")\n    print(f\"Nombre total de séquences dans ce batch: {input_frames.shape[0]}\")\n    \n    for seq_idx in range(input_frames.shape[0]):\n        current_input = input_frames[seq_idx]\n        current_target = target_frames[seq_idx]\n        current_target_name = target_names[seq_idx]\n        \n        # Affichage console\n        target_num = current_target_name.split('_')[1].split('.')[0].zfill(3)\n        input_nums = [str(int(target_num) - len(current_input) + i).zfill(3) for i in range(len(current_input))]\n        \n        print(f\"\\nSéquence {seq_idx + 1}:\")\n        print(f\"• Frames d'entrée ({len(input_nums)}): {', '.join(input_nums)}\")\n        print(f\"• Frame cible: {target_num}\")\n        \n        # Affichage graphique\n        display_sequence_with_target(current_input, current_target, seq_idx, current_target_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pretraitement de Dataloader pour Avune","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass AvenueDataset(Dataset):\n    def __init__(self, root_dir, sequence_length=4, transform=None, overlap=True):\n        self.root_dir = root_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n        \n        # Parcours des dossiers de séquences (02, 03, etc.)\n        for seq_folder in sorted(os.listdir(root_dir)):\n            seq_path = os.path.join(root_dir, seq_folder)\n            if os.path.isdir(seq_path):\n                # Liste des images triées par numéro\n                frames = sorted([f for f in os.listdir(seq_path) if f.endswith('.jpg')],\n                               key=lambda x: int(x.split('.')[0]))\n                frames = [os.path.join(seq_path, f) for f in frames]\n                \n                # Création des séquences\n                if overlap:\n                    for i in range(len(frames) - sequence_length):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n                else:\n                    for i in range(0, len(frames) - sequence_length, sequence_length + 1):\n                        self.sequences.append(frames[i:i + sequence_length + 1])\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        \n        for img_path in frame_paths:\n            img = cv2.imread(img_path)\n            if img is None:\n                raise ValueError(f\"Impossible de charger l'image: {img_path}\")\n                \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            \n            if self.transform:\n                img = self.transform(img)\n                \n            images.append(img)\n        \n        input_frames = torch.stack(images[:self.sequence_length], dim=0)\n        target_frame = images[self.sequence_length]\n        target_name = os.path.basename(frame_paths[-1])\n        \n        return input_frames, target_frame, target_name\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Chemins des données - A ADAPTER selon votre structure exacte\ntrain_root = \"/kaggle/input/avunue-rachid/avenue/training\"\ntest_root = \"/kaggle/input/avunue-rachid/avenue/testing\"\n\n# Datasets\ntrain_dataset = AvenueDataset(\n    root_dir=train_root,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\ntest_dataset = AvenueDataset(\n    root_dir=test_root,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\n# DataLoaders\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ndef display_sequence(input_frames, target_frame, sequence_idx, target_name):\n    \"\"\"\n    Affiche une séquence de frames et la frame cible\n    \"\"\"\n    # Conversion des tenseurs en numpy arrays\n    input_frames = input_frames.cpu().numpy()\n    target_frame = target_frame.cpu().numpy()\n    \n    # Réorganisation des dimensions et dénormalisation\n    input_frames = np.transpose(input_frames, (0, 2, 3, 1))\n    target_frame = np.transpose(target_frame, (1, 2, 0))\n    \n    input_frames = (input_frames + 1) / 2\n    target_frame = (target_frame + 1) / 2\n    \n    # Numéros des frames\n    target_num = target_name.split('.')[0]\n    input_nums = [str(int(target_num) - len(input_frames) + i).zfill(4) \n                 for i in range(len(input_frames))]\n    \n    # Création de la figure\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(f\"Séquence {sequence_idx + 1} - Target: {target_num}\", fontsize=14)\n    \n    # Affichage des frames d'entrée\n    for i, frame in enumerate(input_frames):\n        plt.subplot(1, len(input_frames) + 1, i + 1)\n        plt.imshow(frame)\n        plt.title(f\"Frame {input_nums[i]}\")\n        plt.axis('off')\n    \n    # Affichage de la cible\n    plt.subplot(1, len(input_frames) + 1, len(input_frames) + 1)\n    plt.imshow(target_frame)\n    plt.title(f\"Target {target_num}\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualisation des premières séquences\nNUM_BATCHES_TO_DISPLAY = 2\n\nfor batch_idx, (inputs, targets, names) in enumerate(train_dataloader):\n    if batch_idx >= NUM_BATCHES_TO_DISPLAY:\n        break\n    \n    print(f\"\\n=== Batch {batch_idx + 1} ===\")\n    print(f\"Input shape: {inputs.shape}\")\n    print(f\"Target shape: {targets.shape}\")\n    print(f\"Target names: {names}\")\n    \n    for seq_idx in range(inputs.shape[0]):\n        display_sequence(\n            inputs[seq_idx], \n            targets[seq_idx], \n            seq_idx, \n            names[seq_idx]\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Definition de model","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        self.norm = nn.LayerNorm(dim)\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return self.norm(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ViViT(nn.Module):\n    def __init__(self, image_size, patch_size, num_frames, dim=512, depth=4, heads=3, pool='cls', in_channels=512,\n                 dim_head=64, dropout=0., emb_dropout=0., scale_dim=4, depth_spatial=3, depth_temporal=1):\n        super().__init__()\n\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_frames = num_frames\n        self.in_channels = in_channels\n        self.dim = dim\n        self.depth_spatial = depth_spatial\n        self.depth_temporal = depth_temporal\n\n        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n        self.num_patches = (image_size // patch_size) ** 2\n        self.patch_dim = in_channels * patch_size ** 2  # chaque patch aplati\n\n        # 🔄 PATCH EMBEDDING\n        \n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n            nn.Linear(self.patch_dim, self.dim)  # (b, t, Np, D)\n        )\n\n        # 🔄 TEMPORAL: (b, Np, t+1, d)\n        self.temporal_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.num_frames + 1, self.dim))\n        self.temporal_token = nn.Parameter(torch.randn(1, 1, 1, self.dim))  # (1, 1, 1, d)\n        self.temporal_transformer = Transformer(self.dim, self.depth_temporal, heads, dim_head, dim * scale_dim, dropout)\n\n        # 🔄 SPATIAL\n        self.spatial_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n        self.space_transformer = Transformer(self.dim, self.depth_spatial, heads, dim_head, dim * scale_dim, dropout)\n\n        self.dropout = nn.Dropout(emb_dropout)\n        self.pool = pool\n \n    def forward(self, x):\n        # 🔸 Shape d'entrée : (b, t, c, h, w) = (4, 4, 512, 32, 32)\n        # print('input of vivit :', x.shape)\n        # print('before patches embedding : ')\n        x = self.to_patch_embedding(x)  # (b, t, Np, d)\n        # print('after patch embed : ', x.shape)\n\n        b, t, n, d = x.shape  # t=4, n=256, d=512\n\n        # 🔹 Transpose pour regrouper par patch : → (b, n, t, d)\n        x = x.permute(0, 2, 1, 3)\n        # print('after  regroupe 4 frames par path : ',x.shape)\n\n        # 🔹 Ajouter un prediction token par patch group : shape → (b, n, t+1, d)\n        pred_token = self.temporal_token.expand(b, n, 1, d)\n        x = torch.cat((pred_token, x), dim=2)\n\n        # 🔹 Ajouter position embedding temporel\n        x = x + self.temporal_pos_embedding[:, :, :x.shape[2], :]\n        # print('afeter adding position embedding : ',x.shape)\n        x = self.dropout(x)\n        # print ('after dropout')\n\n        # print('temporal transformer input : ', x.shape)  # (b, n, t+1, d)\n\n        # 🔹 Appliquer transformer temporel par groupe (fusionner batch et Np)\n        x = rearrange(x, 'b n t d -> (b n) t d')\n        x = self.temporal_transformer(x)\n        x = rearrange(x, '(b n) t d -> b n t d', b=b)\n        # print('after temporal transformer : ',x.shape)\n        # 🔹 Récupérer le token de prédiction (position 0)\n        x = x[:, :, 0, :]  # (b, n, d)\n        # print('after temporal transformer : ',x.shape)\n        # print('start embedding spatial')\n        # 🔹 Ajouter embedding spatial\n        x = x + self.spatial_pos_embedding\n        # print('spatial transformer input : ', x.shape)\n\n        # 🔹 Transformer spatial\n        x = self.space_transformer(x)\n        # print('after spatial transformer : ',x.shape)\n        # print(self.image_size)\n\n        # 🔹 Reshape final en feature map (b, c, h, w)\n        # x = rearrange(x, 'b (h w) c -> b c h w', h=self.image_size // self.patch_size)\n        x = rearrange(x, 'b (h w) c -> b c h w', h=self.image_size//2,w=self.image_size//2)\n\n        # print ('after reshaping : ',x.shape)\n\n\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\n\n# class FlowNet(nn.Module):\n#     \"\"\"Module simplifié pour calculer le flux optique entre deux frames\"\"\"\n#     def __init__(self):\n#         super(FlowNet, self).__init__()\n        \n#         # Architecture simple pour estimer le flux optique\n#         self.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3)\n#         self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n#         self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2)\n#         self.conv4 = nn.Conv2d(256, 2, kernel_size=3, stride=1, padding=1)\n        \n#     def forward(self, frame1, frame2):\n#         # Concaténer les deux frames le long de la dimension des canaux\n#         x = torch.cat([frame1, frame2], dim=1)\n        \n#         # Passer à travers les couches\n#         x = F.relu(self.conv1(x))\n#         x = F.relu(self.conv2(x))\n#         x = F.relu(self.conv3(x))\n#         flow = self.conv4(x)\n        \n#         # Redimensionner le flux à la taille originale\n#         flow = F.interpolate(flow, scale_factor=4, mode='bilinear', align_corners=True)\n        \n#         return flow\n\nclass ImprovedFlowNet(nn.Module):\n    \"\"\"Module amélioré pour calculer le flux optique\"\"\"\n    def __init__(self):\n        super(ImprovedFlowNet, self).__init__()\n        \n        # Première partie - extraction de features\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n        )\n        \n        # Deuxième partie - estimation du flux\n        self.flow_estimator = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),\n        )\n        \n        # Module de raffinement\n        self.refinement = nn.Sequential(\n            nn.Conv2d(2, 16, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(16, 2, kernel_size=3, stride=1, padding=1),\n        )\n        \n    def forward(self, frame1, frame2):\n        # Concaténation des frames\n        x = torch.cat([frame1, frame2], dim=1)\n        \n        # Extraction des features\n        features = self.feature_extractor(x)\n        \n        # Estimation du flux\n        flow = self.flow_estimator(features)\n        \n        # Raffinement du flux\n        flow = self.refinement(flow)\n        \n        # Upsampling pour correspondre à la taille d'entrée\n        flow = F.interpolate(flow, scale_factor=4, mode='bilinear', align_corners=True)\n        \n        return flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransAnomaly(nn.Module):\n    def __init__(self, batch_size=4, num_frames=4):\n        super(TransAnomaly, self).__init__()\n        self.batch_size = batch_size\n        self.num_frames = num_frames\n        self.channels_1 = 64\n        self.channels_2 = 128\n        self.channels_3 = 256\n        self.channels_4 = 512\n        \n        # Encoder\n        self.contracting_11 = self.conv_block(in_channels=3, out_channels=self.channels_1)\n        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_21 = self.conv_block(in_channels=self.channels_1, out_channels=self.channels_2)\n        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_31 = self.conv_block(in_channels=self.channels_2, out_channels=self.channels_3)\n        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.contracting_41 = self.conv_block(in_channels=self.channels_3, out_channels=self.channels_4)\n\n        # Residual connections\n        self.residual_14 = nn.Conv2d(in_channels=self.channels_1*self.num_frames, out_channels=self.channels_1, kernel_size=3, stride=1, padding=1)\n        self.residual_23 = nn.Conv2d(in_channels=self.channels_2*self.num_frames, out_channels=self.channels_2, kernel_size=3, stride=1, padding=1)\n        self.residual_32 = nn.Conv2d(in_channels=self.channels_3*self.num_frames, out_channels=self.channels_3, kernel_size=3, stride=1, padding=1)\n        self.residual_41 = nn.Conv2d(in_channels=self.channels_4*self.num_frames, out_channels=self.channels_4, kernel_size=3, stride=1, padding=1)\n\n        # ViViT layer\n        self.middle = ViViT(image_size=32, patch_size=2, num_frames=self.num_frames, in_channels=512)\n\n        # Decoder\n        self.expansive_11 = nn.ConvTranspose2d(in_channels=self.channels_4, out_channels=self.channels_4, kernel_size=3, stride=2, padding=1, output_padding=1) \n        self.expansive_12 = self.conv_block(in_channels=self.channels_4*2, out_channels=self.channels_4)\n        self.expansive_21 = nn.ConvTranspose2d(in_channels=self.channels_4, out_channels=self.channels_3, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_22 = self.conv_block(in_channels=self.channels_3*2, out_channels=self.channels_3)\n        self.expansive_31 = nn.ConvTranspose2d(in_channels=self.channels_3, out_channels=self.channels_2, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_32 = self.conv_block(in_channels=self.channels_2*2, out_channels=self.channels_2)\n        self.expansive_41 = nn.ConvTranspose2d(in_channels=self.channels_2, out_channels=self.channels_1, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.expansive_42 = self.conv_block(in_channels=self.channels_1*2, out_channels=self.channels_1)\n        self.output = nn.Conv2d(in_channels=self.channels_1, out_channels=3, kernel_size=3, stride=1, padding=1)\n        \n        # Module de flux optique\n        # self.flownet = FlowNet()\n        self.flownet = ImprovedFlowNet()\n\n        \n        \n    def conv_block(self, in_channels, out_channels):\n        block = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels)\n        )\n        return block\n    \n    def compute_optical_flow_loss(self, pred_frame, target_frame, prev_frame):\n        \"\"\"\n        Calcule la perte de flux optique entre:\n        1. Le flux entre la prédiction et la frame précédente\n        2. Le flux entre la cible et la frame précédente\n        \"\"\"\n        # Calculer les deux flux optiques\n        flow_pred = self.flownet(prev_frame, pred_frame)\n        flow_target = self.flownet(prev_frame, target_frame)\n        \n        # Calculer la différence L1 entre les deux flux\n        loss = F.l1_loss(flow_pred, flow_target)\n        \n        return loss\n    \n    def forward(self, frames):\n        # frames.shape = (batch, num_frames, c, h, w)\n        batch_size = frames.shape[0]\n        \n        ########## Encoding #######\n        tmp_frames = rearrange(frames, 'b t c h w -> (b t) c h w')\n        \n        contracting_11_out = self.contracting_11(tmp_frames)\n        contracting_12_out = self.contracting_12(contracting_11_out)\n        contracting_21_out = self.contracting_21(contracting_12_out)\n        contracting_22_out = self.contracting_22(contracting_21_out)\n        contracting_31_out = self.contracting_31(contracting_22_out)\n        contracting_32_out = self.contracting_32(contracting_31_out)\n        contracting_41_out = self.contracting_41(contracting_32_out)\n        \n        ####### ViViT layer ########\n        vivit_input = rearrange(contracting_41_out, '(b t) c h w -> b t c h w', b=batch_size)\n        middle_out = self.middle(vivit_input)\n        \n        ######### Residual connections #####\n        residual_14_out = rearrange(contracting_11_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_14_out = self.residual_14(residual_14_out)\n\n        residual_23_out = rearrange(contracting_21_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_23_out = self.residual_23(residual_23_out)\n\n        residual_32_out = rearrange(contracting_31_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_32_out = self.residual_32(residual_32_out)\n\n        residual_41_out = rearrange(contracting_41_out, '(b t) c h w -> b (t c) h w', b=batch_size)\n        residual_41_out = self.residual_41(residual_41_out)\n        \n        ####### Decoding ##########\n        expansive_11_out = self.expansive_11(middle_out)\n        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, residual_41_out), dim=1))\n        expansive_21_out = self.expansive_21(expansive_12_out)\n        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, residual_32_out), dim=1))\n        expansive_31_out = self.expansive_31(expansive_22_out)\n        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, residual_23_out), dim=1))\n        expansive_41_out = self.expansive_41(expansive_32_out)\n        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, residual_14_out), dim=1))\n        output_out = self.output(expansive_42_out)\n        \n        return output_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## les fonction de loss","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class GradientLoss(nn.Module):\n#     \"\"\"Perte de gradient pour améliorer la netteté des images prédites\"\"\"\n#     def __init__(self):\n#         super(GradientLoss, self).__init__()\n        \n#     def forward(self, pred_frame, target_frame):\n#         \"\"\"\n#         Calcule la différence entre les gradients des images prédites et cibles\n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image prédite\n#             target_frame: tensor [B, C, H, W] - image cible\n#         Returns:\n#             loss: tensor scalaire - perte de gradient\n#         \"\"\"\n#         # Calcul des gradients horizontaux et verticaux pour l'image prédite\n#         pred_dx = pred_frame[:, :, :, :-1] - pred_frame[:, :, :, 1:]  # Gradient horizontal\n#         pred_dy = pred_frame[:, :, :-1, :] - pred_frame[:, :, 1:, :]  # Gradient vertical\n        \n#         # Calcul des gradients horizontaux et verticaux pour l'image cible\n#         target_dx = target_frame[:, :, :, :-1] - target_frame[:, :, :, 1:]\n#         target_dy = target_frame[:, :, :-1, :] - target_frame[:, :, 1:, :]\n        \n#         # Calcul des pertes L1 pour les gradients\n#         loss_x = F.l1_loss(pred_dx, target_dx)\n#         loss_y = F.l1_loss(pred_dy, target_dy)\n        \n#         return loss_x + loss_y\n\n# class OpticalFlowLoss(nn.Module):\n#     \"\"\"Perte de flux optique pour la cohérence temporelle\"\"\"\n#     def __init__(self):\n#         super(OpticalFlowLoss, self).__init__()\n        \n#     def forward(self, pred_frame, target_frame, prev_frame, flownet):\n#         \"\"\"\n#         Calcule la perte de flux optique entre:\n#         1. Le flux entre la prédiction et la frame précédente\n#         2. Le flux entre la cible et la frame précédente\n        \n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image prédite\n#             target_frame: tensor [B, C, H, W] - image cible\n#             prev_frame: tensor [B, C, H, W] - frame précédente\n#             flownet: nn.Module - réseau pour calculer le flux optique\n#         Returns:\n#             loss: tensor scalaire - perte de flux optique\n#         \"\"\"\n#         # Calculer les deux flux optiques\n#         flow_pred = flownet(prev_frame, pred_frame)\n#         flow_target = flownet(prev_frame, target_frame)\n        \n#         # Calculer la différence L1 entre les deux flux\n#         loss = F.l1_loss(flow_pred, flow_target)\n        \n#         return loss\n\n# class CombinedLoss(nn.Module):\n#     \"\"\"Combinaison des différentes pertes avec pondération\"\"\"\n#     def __init__(self, identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05):\n#         \"\"\"\n#         Args:\n#             identity_weight: float - poids pour la perte d'identité (MSE)\n#             gradient_weight: float - poids pour la perte de gradient\n#             flow_weight: float - poids pour la perte de flux optique\n#         \"\"\"\n#         super(CombinedLoss, self).__init__()\n#         self.identity_loss = nn.MSELoss()\n#         self.gradient_loss = GradientLoss()\n#         self.optical_flow_loss = OpticalFlowLoss()\n#         self.identity_weight = identity_weight\n#         self.gradient_weight = gradient_weight\n#         self.flow_weight = flow_weight\n        \n#     def forward(self, pred_frame, target_frame, prev_frame, flownet):\n#         \"\"\"\n#         Calcule la perte combinée\n        \n#         Args:\n#             pred_frame: tensor [B, C, H, W] - image prédite\n#             target_frame: tensor [B, C, H, W] - image cible\n#             prev_frame: tensor [B, C, H, W] - frame précédente\n#             flownet: nn.Module - réseau pour calculer le flux optique\n#         Returns:\n#             loss: tensor scalaire - perte combinée pondérée\n#         \"\"\"\n#         # Perte d'identité (MSE)\n#         identity = self.identity_loss(pred_frame, target_frame) * self.identity_weight\n        \n#         # Perte de gradient\n#         gradient = self.gradient_loss(pred_frame, target_frame) * self.gradient_weight\n        \n#         # Perte de flux optique\n#         flow = self.optical_flow_loss(pred_frame, target_frame, prev_frame, flownet) * self.flow_weight\n        \n#         return identity + gradient + flow\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass GradientLoss(nn.Module):\n    \"\"\"Perte de gradient améliorée avec gestion des bords\"\"\"\n    def __init__(self, epsilon=1e-6):\n        super(GradientLoss, self).__init__()\n        self.epsilon = epsilon\n        \n        # Initialisation des noyaux de convolution une seule fois\n        self.register_buffer('kernel_x', torch.tensor([[[[1.0, -1.0]]]]))\n        self.register_buffer('kernel_y', torch.tensor([[[[1.0], [-1.0]]]]))\n        \n    def forward(self, pred_frame, target_frame):\n        \"\"\"\n        Version corrigée avec :\n        - Conversion automatique des types\n        - Gestion des bords optimisée\n        \"\"\"\n        # Vérification des dimensions\n        if pred_frame.dim() != 4 or target_frame.dim() != 4:\n            raise ValueError(\"Les inputs doivent être de dimension [B, C, H, W]\")\n            \n        # Calcul des gradients\n        pred_dx = F.conv2d(pred_frame, \n                          self.kernel_x.repeat(pred_frame.size(1), 1, 1, 1),\n                          padding=0, \n                          groups=pred_frame.size(1))\n        \n        pred_dy = F.conv2d(pred_frame, \n                          self.kernel_y.repeat(pred_frame.size(1), 1, 1, 1),\n                          padding=0, \n                          groups=pred_frame.size(1))\n        \n        target_dx = F.conv2d(target_frame, \n                            self.kernel_x.repeat(target_frame.size(1), 1, 1, 1),\n                            padding=0, \n                            groups=target_frame.size(1))\n        \n        target_dy = F.conv2d(target_frame, \n                            self.kernel_y.repeat(target_frame.size(1), 1, 1, 1),\n                            padding=0, \n                            groups=target_frame.size(1))\n        \n        # Calcul des pertes L1\n        loss_x = F.l1_loss(pred_dx, target_dx)\n        loss_y = F.l1_loss(pred_dy, target_dy)\n        \n        return (loss_x + loss_y) / 2\n\nclass OpticalFlowLoss(nn.Module):\n    \"\"\"Perte de flux optique simplifiée mais efficace\"\"\"\n    def __init__(self):\n        super(OpticalFlowLoss, self).__init__()\n        \n    def forward(self, pred_frame, target_frame, prev_frame, flownet):\n        # Calcul des flux\n        flow_pred = flownet(prev_frame, pred_frame)\n        flow_target = flownet(prev_frame, target_frame)\n        \n        # Perte L1 simple\n        return F.l1_loss(flow_pred, flow_target)\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Version simplifiée mais robuste de CombinedLoss\"\"\"\n    def __init__(self, identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05):\n        super(CombinedLoss, self).__init__()\n        self.identity_loss = nn.MSELoss()\n        self.gradient_loss = GradientLoss()\n        self.optical_flow_loss = OpticalFlowLoss()\n        \n        # Poids fixes (version simplifiée)\n        self.identity_weight = identity_weight\n        self.gradient_weight = gradient_weight\n        self.flow_weight = flow_weight\n        \n    def forward(self, pred_frame, target_frame, prev_frame, flownet):\n        # Calcul des pertes\n        identity = self.identity_loss(pred_frame, target_frame) * self.identity_weight\n        gradient = self.gradient_loss(pred_frame, target_frame) * self.gradient_weight\n        flow = self.optical_flow_loss(pred_frame, target_frame, prev_frame, flownet) * self.flow_weight\n        \n        return identity + gradient + flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Entrainement","metadata":{}},{"cell_type":"code","source":"# Fonction d'accuracy et d'entraînement\ndef pixel_accuracy(preds, targets):\n    preds = torch.sigmoid(preds)\n    preds = (preds > 0.5).float()\n    targets = (targets > 0.5).float()\n    correct = (preds == targets).float()\n    return correct.sum() / correct.numel()\n\ndef train(model, dataloader, optimizer, epoch, log_interval=200):\n    model.train()\n    total_loss, total_acc = 0.0, 0.0\n    \n    for batch_idx, (input_sequences, target_frames, _) in enumerate(dataloader):\n        input_sequences = input_sequences.to(device)\n        target_frames = target_frames.to(device)\n        \n        # La dernière frame d'entrée est la frame précédente\n        prev_frames = input_sequences[:, -1, :, :, :]\n        \n        optimizer.zero_grad()\n        predicted_frames = model(input_sequences)\n        \n        # Utilisez model.flownet au lieu de flownet\n        loss = criterion(predicted_frames, target_frames, prev_frames, model.flownet)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        total_acc += pixel_accuracy(predicted_frames, target_frames).item()\n        \n        if batch_idx % log_interval == 0:\n            avg_loss = total_loss / (batch_idx + 1)\n            avg_acc = total_acc / (batch_idx + 1)\n            print(f\"Epoch {epoch} | Batch {batch_idx}/{len(dataloader)} | \"\n                  f\"Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}\")\n\n    avg_loss = total_loss / len(dataloader)\n    avg_acc = total_acc / len(dataloader)\n    print(f\"\\nEpoch {epoch} Summary: Avg Loss: {avg_loss:.4f} | Avg Acc: {avg_acc:.4f}\\n\")\n    return avg_loss, avg_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n\n# Initialisation du modèle\nmodel = TransAnomaly().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\ncriterion = CombinedLoss(identity_weight=1.0, gradient_weight=1.0, flow_weight=0.05).to(device)\n\n# Boucle d'entraînement\nlosses, accuracies = [], []\nEPOCHS = 6\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_loss, epoch_acc = train(model, train_dataloader, optimizer, epoch)\n    losses.append(epoch_loss)\n    accuracies.append(epoch_acc)\n    print(f\"[EPOCH: {epoch}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n\n# ➕ Plotting\nplt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\nplt.plot(losses, label='Loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss par Epoch\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(accuracies, label='Accuracy', color='green')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy par Epoch\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation avec ROC courb","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns  # Pour une meilleure visualisation de la matrice de confusion\n\ndef calculate_psnr(img1, img2, max_pixel=1.0):\n    \"\"\"Calcule le PSNR entre deux images (supposées normalisées entre -1 et 1).\"\"\"\n    img1 = (img1 + 1) / 2  # Conversion [-1, 1] -> [0, 1]\n    img2 = (img2 + 1) / 2\n    mse = torch.mean((img1 - img2) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 10 * torch.log10(max_pixel ** 2 / mse).item()\n\ndef evaluate_model(model, dataloader, device, window_size=64, stride=32):\n    model.eval()\n    psnrs = []\n    \n    with torch.no_grad():\n        for input_frames, target_frames, _ in dataloader:\n            input_frames = input_frames.to(device)\n            target_frames = target_frames.to(device)\n            predicted_frames = model(input_frames)\n            for i in range(predicted_frames.shape[0]):\n                psnr = calculate_psnr(predicted_frames[i], target_frames[i])\n                psnrs.append(psnr)\n    \n    psnrs = np.array(psnrs)\n    scores = (psnrs - psnrs.min()) / (psnrs.max() - psnrs.min())\n    fake_labels = np.zeros_like(scores)\n    fake_labels[np.argsort(scores)[:len(scores)//10]] = 1  # 10% lowest PSNR = anomalies\n    \n    fpr, tpr, thresholds = roc_curve(fake_labels, 1 - scores)  # Inverser car PSNR bas = anomalie\n    roc_auc = auc(fpr, tpr)\n    optimal_idx = np.argmax(tpr - fpr)\n    optimal_threshold = thresholds[optimal_idx]\n    predictions = (1 - scores) > optimal_threshold\n    tn, fp, fn, tp = confusion_matrix(fake_labels, predictions).ravel()\n    \n    return roc_auc, (tp, fp, tn, fn), scores, fpr, tpr, confusion_matrix(fake_labels, predictions)  # Ajout de la matrice de confusion complète\n\n# Utilisation\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Évaluation et calcul de l'AUC\nroc_auc, (tp, fp, tn, fn), scores, fpr, tpr, conf_matrix = evaluate_model(model, test_dataloader, device)\n\n# Affichage des résultats\nprint(f\"[AUC] = {roc_auc:.4f}\")\nprint(f\"Matrice de confusion: TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\nprint(f\"Precision = {tp / (tp + fp):.4f}, Recall = {tp / (tp + fn):.4f}\")\n\n# Tracé de la courbe ROC\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\n# Tracé de la matrice de confusion\nplt.subplot(1, 2, 2)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Normal', 'Anomaly'], \n            yticklabels=['Normal', 'Anomaly'])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation pour un seul video","metadata":{}},{"cell_type":"code","source":"## Extraire les frames d'un seul video","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\n\n# Chemin de la vidéo à traiter\nvideo_path = \"/kaggle/input/anomalous-action-detection-dataset/Anomalous Action Detection Dataset( Ano-AAD)/abnormal class/Fighting/46.mp4\"\noutput_folder=\"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale video\"\n\n# S'assurer que le dossier de sortie existe\nos.makedirs(output_folder, exist_ok=True)\n\n# Extraire le nom de la vidéo sans extension\nvideo_name = os.path.splitext(os.path.basename(video_path))[0]\nvideo_output_folder = os.path.join(output_folder, video_name)\nos.makedirs(video_output_folder, exist_ok=True)\n\n# Ouvrir la vidéo\ncap = cv2.VideoCapture(video_path)\n    \n# Vérifier si la vidéo s'ouvre correctement\nif not cap.isOpened():\n    print(f\"❌ Erreur de chargement : {video_path}\")\nelse:\n    print(f\"🎥 Traitement de {video_path}\")\n\n    # Extraire et sauvegarder chaque frame\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # Fin de la vidéo\n\n        # Sauvegarder l'image en JPG\n        frame_filename = os.path.join(video_output_folder, f\"frame_{frame_idx:04d}.jpg\")\n        cv2.imwrite(frame_filename, frame)\n        frame_idx += 1\n\n    cap.release()\n    print(f\"✔ {frame_idx} Frames sauvegardées dans : {video_output_folder}\")\n    print(\"✅ Extraction terminée !\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## pour cadrer le nombre de frames  utiliser pour chaque video","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Dossier source de la vidéo (images/frames)\ninput_video_folder = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale video/46\"  # <-- Remplace par le nom du dossier de ta vidéo\noutput_folder = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale videok\"\n\n\n# Nombre maximum de frames à copier\nMAX_FRAMES = 450\n\n# S'assurer que le dossier de sortie existe\nos.makedirs(output_folder, exist_ok=True)\n\n# Lister et trier les frames\nframes = sorted(os.listdir(input_video_folder))\n\nframes_to_copy = frames[:MAX_FRAMES]\n\n# Copier les frames\nfor frame in frames_to_copy:\n    src = os.path.join(input_video_folder, frame)\n    dst = os.path.join(output_folder, frame)\n    shutil.copy(src, dst)\n\nprint(f\"✅ {len(frames_to_copy)} frames copiées depuis {input_video_folder} vers {output_folder}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DataLoader pour un seule video :","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass SingleVideoDataset(Dataset):\n    def __init__(self, frames_dir, sequence_length=4, transform=None, overlap=True):\n        self.frames_dir = frames_dir\n        self.sequence_length = sequence_length\n        self.transform = transform\n        self.overlap = overlap\n        self.sequences = []\n\n        # Charger et trier les frames\n        self.frames = sorted(\n            [f for f in os.listdir(frames_dir) if f.startswith('frame_')],\n            key=lambda x: int(x.split('_')[1].split('.')[0]))\n        \n        # Calculer le nombre maximal de séquences complètes\n        max_sequences = len(self.frames) - sequence_length\n        if max_sequences <= 0:\n            raise ValueError(\"Pas assez de frames pour former une séquence !\")\n        \n        # Générer les séquences avec troncature finale\n        if overlap:\n            self.sequences = [\n                [os.path.join(frames_dir, self.frames[j]) \n                 for j in range(i, i + sequence_length + 1)]\n                for i in range(max_sequences)\n            ]\n        else:\n            step = sequence_length + 1\n            self.sequences = [\n                [os.path.join(frames_dir, self.frames[j]) \n                 for j in range(i, i + sequence_length + 1)]\n                for i in range(0, max_sequences, step)\n            ]\n        \n        # Tronquer pour avoir un nombre de séquences divisible par le batch_size\n        self.total_sequences = len(self.sequences)\n\n    def __len__(self):\n        return self.total_sequences\n\n    def __getitem__(self, idx):\n        frame_paths = self.sequences[idx]\n        images = []\n        \n        for path in frame_paths:\n            img = cv2.imread(path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.resize(img, (256, 256))\n            \n            if self.transform:\n                img = self.transform(img)\n                \n            images.append(img)\n\n        input_frames = torch.stack(images[:-1], dim=0)  # Les N premières frames\n        target_frame = images[-1]  # La dernière frame\n        return input_frames, target_frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Définir les transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Création du Dataset pour une seule vidéo\nvideo_frames_dir = \"/kaggle/working/Anomalous Action Detection Dataset/NormaleAction1/abnormale videok\" \ndataset = SingleVideoDataset(\n    frames_dir=video_frames_dir,\n    sequence_length=4,\n    transform=transform,\n    overlap=True\n)\n\n# Création du DataLoader\nsignal_dataloader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=False,  # Garder l'ordre temporel\n    num_workers=2,\n    pin_memory=True,\n    drop_last=True  # <-- Ajoutez cette ligne\n\n)\n\n# # Exemple d'utilisation\n# for inputs, targets in signal_dataloader:\n#     print(f\"Batch input shape: {inputs.shape}\")  # [4, 4, 3, 256, 256]\n#     print(f\"Batch target shape: {targets.shape}\")  # [4, 3, 256, 256]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation d'un seul video long ou plusieurs video moyen ( le graphe de score d'anomalie au cours de temps)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n\ndef calculate_psnr(pred, target, max_val=1.0):\n    \"\"\"Calcule le PSNR entre une frame prédite et la frame réelle.\"\"\"\n    pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n    target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n    return psnr(target_np, pred_np, data_range=max_val)\n\ndef sliding_window_psnr(pred, target, window_size=64, stride=32, max_val=1.0):\n    \"\"\"Calcule le PSNR sur les patches avec les plus grandes erreurs (sliding window).\"\"\"\n    h, w = pred.shape[-2], pred.shape[-1]\n    mse_patches = []\n    \n    for i in range(0, h - window_size + 1, stride):\n        for j in range(0, w - window_size + 1, stride):\n            patch_pred = pred[..., i:i+window_size, j:j+window_size]\n            patch_target = target[..., i:i+window_size, j:j+window_size]\n            mse = torch.mean((patch_pred - patch_target) ** 2).item()\n            mse_patches.append(mse)\n    \n    if not mse_patches:\n        return 0.0\n    \n    # Prend les p patches avec les plus grandes MSE (p = moitié)\n    p = len(mse_patches) // 2\n    top_mse = sorted(mse_patches, reverse=True)[:p]\n    avg_mse = np.mean(top_mse)\n    return 10 * np.log10(max_val ** 2 / avg_mse) if avg_mse > 0 else 100.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# without opticale flow\n\n# def evaluate_anomaly_unlabeled(model, dataloader, device, use_sliding_window=True, window_size=64, stride=32):\n#     model.eval()\n#     psnr_scores = []\n#     frame_predictions = []\n\n#     with torch.no_grad():\n#         for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):  # Ajoutez _ pour ignorer le 3ème élément\n#             input_sequences = input_sequences.to(device)\n#             target_frames = target_frames.to(device)\n#             predicted_frames = model(input_sequences)\n            \n#             for pred, target in zip(predicted_frames, target_frames):\n#                 if use_sliding_window:\n#                     psnr_score = sliding_window_psnr(pred, target, window_size, stride)\n#                 else:\n#                     psnr_score = calculate_psnr(pred, target)\n#                 psnr_scores.append(psnr_score)\n#                 frame_predictions.append((pred.cpu(), target.cpu()))\n\n#     min_psnr, max_psnr = min(psnr_scores), max(psnr_scores)\n#     regularity_scores = [(psnr - min_psnr) / (max_psnr - min_psnr + 1e-6) for psnr in psnr_scores]\n    \n#     return regularity_scores, frame_predictions\n\n#with opticale flow\n\ndef evaluate_anomaly_unlabeled(model, dataloader, device, use_sliding_window=True, window_size=64, stride=32):\n    model.eval()\n    psnr_scores = []\n    frame_predictions = []\n\n    with torch.no_grad():\n        for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n            input_sequences = input_sequences.to(device)\n            target_frames = target_frames.to(device)\n            \n            # La dernière frame d'entrée est la frame précédente\n            prev_frames = input_sequences[:, -1, :, :, :]\n            \n            predicted_frames = model(input_sequences)\n            \n            # Calcul du flux optique entre la frame précédente et la prédiction\n            flow_pred = model.flownet(prev_frames, predicted_frames)\n            \n            for pred, target, flow in zip(predicted_frames, target_frames, flow_pred):\n                if use_sliding_window:\n                    psnr_score = sliding_window_psnr(pred, target, window_size, stride)\n                else:\n                    psnr_score = calculate_psnr(pred, target)\n                psnr_scores.append(psnr_score)\n                frame_predictions.append((pred.cpu(), target.cpu(), flow.cpu()))\n\n    min_psnr, max_psnr = min(psnr_scores), max(psnr_scores)\n    regularity_scores = [(psnr - min_psnr) / (max_psnr - min_psnr + 1e-6) for psnr in psnr_scores]\n    \n    return regularity_scores, frame_predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_anomalies(scores, threshold, num_frames_to_plot=5):\n    anomalous_indices = np.where(np.array(scores) < threshold)[0]\n    plt.figure(figsize=(15, 5))\n    plt.plot(scores, label='Scores de régularité')\n    plt.axhline(y=threshold, color='r', linestyle='--', label='Seuil d\\'anomalie')\n    plt.scatter(anomalous_indices[:num_frames_to_plot], \n                [scores[i] for i in anomalous_indices[:num_frames_to_plot]], \n                color='red', label='Frames anormales')\n    plt.legend()\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"Score (0=anomalie, 1=normal)\")\n    plt.title(\"Détection d'anomalies\")\n    plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## apparence d'anomalie a l'aide de optical flow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport cv2\n\n# 1. Scores\nscores, frames = evaluate_anomaly_unlabeled(model, signal_dataloader, device)\n\n# 2. Seuil\nthreshold = np.percentile(scores, 5)\n\n# 3. Visualisation\nplot_anomalies(scores, threshold)\n\n# 4. Indices anormaux\nanomalous_indices = np.where(np.array(scores) < threshold)[0]\n\n\n# 5. Affichage des 10 premières anomalies\nfor idx in anomalous_indices[:30]:\n    pred, target, _ = frames[idx]\n\n    def prepare_image(tensor):\n        img = tensor.numpy()\n        if len(img.shape) == 3 and img.shape[0] == 3:\n            img = img.transpose(1, 2, 0)  # (C, H, W) → (H, W, C)\n        img = (img - img.min()) / (img.max() - img.min() + 1e-6)  # Normalisation [0, 1]\n        return np.clip(img, 0, 1)\n\n    # --- Calcul du masque d'erreur ---\n    diff = torch.abs(pred - target)\n    error_map = diff.mean(dim=0)  # Moyenne sur les canaux RGB\n    norm_error = (error_map - error_map.min()) / (error_map.max() - error_map.min() + 1e-6)\n    blurred_mask = cv2.GaussianBlur(norm_error.numpy(), (31, 31), 10)  # Réduction du flou pour plus de détails\n\n    # --- Création de la heatmap améliorée ---\n    heatmap = cv2.applyColorMap((blurred_mask * 255).astype(np.uint8), cv2.COLORMAP_INFERNO)  # Colormap plus contrastée\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n\n    # Renforcement des couleurs (HSV)\n    hsv = cv2.cvtColor(heatmap, cv2.COLOR_RGB2HSV)\n    hsv[..., 1] = hsv[..., 1] * 1.8  # Saturation augmentée\n    hsv[..., 2] = np.clip(hsv[..., 2] * 1.3, 0, 1)  # Luminosité augmentée\n    heatmap = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n\n    # --- Transparence adaptative ---\n    alpha_mask = np.clip(blurred_mask * 3.5, 0, 1)  # Réduction de la transparence\n    base = np.ones_like(heatmap) * 0.9  # Fond légèrement gris pour mieux contraster\n\n    # Fusion finale\n    transparent_heatmap = heatmap * alpha_mask[..., np.newaxis] + base * (1 - alpha_mask[..., np.newaxis])\n\n    # --- Affichage ---\n    plt.figure(figsize=(18, 6), facecolor='white')\n\n    # Image réelle\n    plt.subplot(1, 3, 1)\n    plt.imshow(prepare_image(target))\n    plt.title(\"Image Réelle\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    # Image prédite\n    plt.subplot(1, 3, 2)\n    plt.imshow(prepare_image(pred))\n    plt.title(f\"Prédiction (score: {scores[idx]:.2f})\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    # Heatmap des anomalies\n    plt.subplot(1, 3, 3)\n    plt.imshow(transparent_heatmap)\n    plt.title(\"Flux Optique - Anomalies\", fontsize=12, pad=10)\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# evaluation avec bounding box    ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom skimage.metrics import peak_signal_noise_ratio as psnr\n\ndef sliding_window_anomaly_detection(pred, target, window_size=64, stride=32, threshold=0.1):\n    \"\"\"Détecte les patches anormaux et retourne leurs coordonnées\"\"\"\n    h, w = pred.shape[-2], pred.shape[-1]\n    anomaly_boxes = []\n    \n    for i in range(0, h - window_size + 1, stride):\n        for j in range(0, w - window_size + 1, stride):\n            patch_pred = pred[..., i:i+window_size, j:j+window_size]\n            patch_target = target[..., i:i+window_size, j:j+window_size]\n            \n            # Calcul de l'erreur MSE pour le patch\n            mse = torch.mean((patch_pred - patch_target) ** 2).item()\n            \n            # Si l'erreur dépasse le seuil, on considère le patch comme anormal\n            if mse > threshold:\n                anomaly_boxes.append((j, i, j+window_size, i+window_size, mse))\n    \n    return anomaly_boxes\n\n# def evaluate_and_visualize_anomalies(model, dataloader, device, window_size=64, stride=32, anomaly_threshold=0.1, display_count=5):\n#     model.eval()\n    \n#     with torch.no_grad():\n#         for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n#             input_sequences = input_sequences.to(device)\n#             target_frames = target_frames.to(device)\n#             predicted_frames = model(input_sequences)\n            \n#             for idx, (pred, target) in enumerate(zip(predicted_frames, target_frames)):\n#                 # Convertir les tenseurs en images numpy\n#                 pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)\n#                 target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n                \n#                 # Normalisation des images\n#                 pred_np = (pred_np - pred_np.min()) / (pred_np.max() - pred_np.min() + 1e-6)\n#                 target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-6)\n                \n#                 # Détection des anomalies par sliding window\n#                 anomaly_boxes = sliding_window_anomaly_detection(pred, target, window_size, stride, anomaly_threshold)\n                \n#                 # Visualisation\n#                 if len(anomaly_boxes) > 0 and idx < display_count:\n#                     plt.figure(figsize=(15, 6))\n                    \n#                     # Afficher l'image originale avec les bounding boxes\n#                     plt.subplot(1, 2, 1)\n#                     plt.imshow(target_np)\n#                     plt.title(\"Image Originale avec Anomalies Détectées\")\n                    \n#                     # Dessiner les bounding boxes autour des anomalies\n#                     for box in anomaly_boxes:\n#                         x1, y1, x2, y2, mse = box\n#                         rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n#                                                linewidth=2, edgecolor='r', facecolor='none')\n#                         plt.gca().add_patch(rect)\n#                         plt.text(x1, y1, f\"{mse:.2f}\", color='white', \n#                                 bbox=dict(facecolor='red', alpha=0.5))\n                    \n#                     # Afficher l'image prédite pour comparaison\n#                     plt.subplot(1, 2, 2)\n#                     plt.imshow(pred_np)\n#                     plt.title(\"Image Prédite\")\n                    \n#                     plt.tight_layout()\n#                     plt.show()\n\n# # Paramètres\n# window_size = 64  # Taille des patches\n# stride = 32       # Pas du sliding window\n# anomaly_threshold = 0.1  # Seuil d'anomalie (à ajuster selon votre cas)\n\n# # Exécution\n# evaluate_and_visualize_anomalies(model, signal_dataloader, device, \n#                                 window_size=window_size, \n#                                 stride=stride, \n#                                 anomaly_threshold=anomaly_threshold)\n\n\ndef evaluate_and_visualize_anomalies(model, dataloader, device, window_size=64, stride=32, \n                                   anomaly_threshold=0.1, max_display=20):  # Ajout du paramètre max_display\n    model.eval()\n    displayed_count = 0  # Compteur pour les frames affichées\n    \n    with torch.no_grad():\n        for batch_idx, (input_sequences, target_frames) in enumerate(dataloader):\n            if displayed_count >= max_display:  # Arrêter après 20 frames\n                break\n                \n            input_sequences = input_sequences.to(device)\n            target_frames = target_frames.to(device)\n            predicted_frames = model(input_sequences)\n            \n            for idx, (pred, target) in enumerate(zip(predicted_frames, target_frames)):\n                if displayed_count >= max_display:  # Vérification à chaque frame\n                    break\n                    \n                # Convertir les tenseurs en images numpy\n                pred_np = pred.detach().cpu().numpy().transpose(1, 2, 0)\n                target_np = target.detach().cpu().numpy().transpose(1, 2, 0)\n                \n                # Normalisation des images\n                pred_np = (pred_np - pred_np.min()) / (pred_np.max() - pred_np.min() + 1e-6)\n                target_np = (target_np - target_np.min()) / (target_np.max() - target_np.min() + 1e-6)\n                \n                # Détection des anomalies par sliding window\n                anomaly_boxes = sliding_window_anomaly_detection(pred, target, window_size, stride, anomaly_threshold)\n                \n                # Visualisation seulement si anomalies détectées\n                if len(anomaly_boxes) > 0:\n                    displayed_count += 1  # Incrémenter le compteur\n                    \n                    plt.figure(figsize=(15, 6))\n                    \n                    # Afficher l'image originale avec les bounding boxes\n                    plt.subplot(1, 2, 1)\n                    plt.imshow(target_np)\n                    plt.title(f\"Frame Anormale {displayed_count}/{max_display}\")\n                    \n                    # Dessiner les bounding boxes autour des anomalies\n                    for box in anomaly_boxes:\n                        x1, y1, x2, y2, mse = box\n                        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n                                               linewidth=2, edgecolor='r', facecolor='none')\n                        plt.gca().add_patch(rect)\n                        plt.text(x1, y1, f\"{mse:.2f}\", color='white', \n                                bbox=dict(facecolor='red', alpha=0.5))\n                    \n                    # Afficher l'image prédite pour comparaison\n                    plt.subplot(1, 2, 2)\n                    plt.imshow(pred_np)\n                    plt.title(\"Prédiction Correspondante\")\n                    \n                    plt.tight_layout()\n                    plt.show()\n\n# Paramètres (inchangés)\nwindow_size = 64\nstride = 32\nanomaly_threshold = 0.1\n\n# Exécution avec affichage des 20 premières frames anormales\nevaluate_and_visualize_anomalies(model, signal_dataloader, device, \n                                window_size=window_size, \n                                stride=stride, \n                                anomaly_threshold=anomaly_threshold,\n                                max_display=20)  # Nouveau paramètre","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}